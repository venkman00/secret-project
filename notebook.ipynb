{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Lens Correction\n",
    "\n",
    "**Approach:** Parametric distortion prediction (k1, k2, k3, cx, cy) using EfficientNet-B3 + differentiable undistortion + test-time optimization.\n",
    "\n",
    "**Setup:** Add the competition data via **Add Data -> Competition -> automatic-lens-correction**. Enable **GPU** in accelerator settings. No internet required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 — Imports & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, glob, json, csv, zipfile, random\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from scipy.optimize import minimize\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "INPUT_DIR = Path('/kaggle/input/automatic-lens-correction')\n",
    "WORK_DIR = Path('/kaggle/working')\n",
    "\n",
    "print(f\"\\nData directory contents:\")\n",
    "for p in sorted(INPUT_DIR.rglob('*')):\n",
    "    if p.is_dir():\n",
    "        n_files = len(list(p.iterdir()))\n",
    "        print(f\"  [DIR]  {p.relative_to(INPUT_DIR)}/ ({n_files} items)\")\n",
    "\n",
    "img_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "all_images = [p for p in INPUT_DIR.rglob('*') if p.suffix.lower() in img_exts]\n",
    "print(f\"\\nTotal images found: {len(all_images)}\")\n",
    "\n",
    "print(\"\\nSample filenames:\")\n",
    "for img in all_images[:5]:\n",
    "    print(f\"  {img.relative_to(INPUT_DIR)}\")\n",
    "\n",
    "if all_images:\n",
    "    sample = cv2.imread(str(all_images[0]))\n",
    "    if sample is not None:\n",
    "        print(f\"\\nSample resolution: {sample.shape[1]}x{sample.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 — Auto-Detect Directory Structure\n",
    "\n",
    "Check the output. If auto-detect fails, uncomment the manual override at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_dirs(data_dir):\n",
    "    data_dir = Path(data_dir)\n",
    "    patterns = [\n",
    "        ('train/distorted', 'train/corrected'),\n",
    "        ('train/input', 'train/target'),\n",
    "        ('train_distorted', 'train_corrected'),\n",
    "        ('distorted', 'corrected'),\n",
    "        ('input', 'target'),\n",
    "        ('train_input', 'train_target'),\n",
    "        ('train/input', 'train/ground_truth'),\n",
    "    ]\n",
    "    dist_dir = corr_dir = test_dir = None\n",
    "    for dd, cd in patterns:\n",
    "        d, c = data_dir / dd, data_dir / cd\n",
    "        if d.exists() and c.exists():\n",
    "            dist_dir, corr_dir = d, c\n",
    "            break\n",
    "    for tp in ['test', 'test_images', 'test_input', 'test/input', 'test/distorted']:\n",
    "        t = data_dir / tp\n",
    "        if t.exists():\n",
    "            test_dir = t\n",
    "            break\n",
    "    if dist_dir is None:\n",
    "        subdirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])\n",
    "        for i, d1 in enumerate(subdirs):\n",
    "            for d2 in subdirs[i+1:]:\n",
    "                f1 = {f.stem for f in d1.glob('*') if f.suffix.lower() in img_exts}\n",
    "                f2 = {f.stem for f in d2.glob('*') if f.suffix.lower() in img_exts}\n",
    "                if len(f1 & f2) > 10:\n",
    "                    dist_dir, corr_dir = d1, d2\n",
    "                    break\n",
    "            if dist_dir: break\n",
    "    if test_dir is None:\n",
    "        for d in sorted(data_dir.iterdir()):\n",
    "            if d.is_dir() and d != dist_dir and d != corr_dir:\n",
    "                if any(f.suffix.lower() in img_exts for f in d.rglob('*')):\n",
    "                    test_dir = d\n",
    "                    break\n",
    "    return dist_dir, corr_dir, test_dir\n",
    "\n",
    "DIST_DIR, CORR_DIR, TEST_DIR = find_dirs(INPUT_DIR)\n",
    "print(f\"Distorted dir: {DIST_DIR}\")\n",
    "print(f\"Corrected dir: {CORR_DIR}\")\n",
    "print(f\"Test dir:      {TEST_DIR}\")\n",
    "if DIST_DIR:\n",
    "    print(f\"\\nDistorted images: {len([f for f in DIST_DIR.rglob('*') if f.suffix.lower() in img_exts])}\")\n",
    "if CORR_DIR:\n",
    "    print(f\"Corrected images: {len([f for f in CORR_DIR.rglob('*') if f.suffix.lower() in img_exts])}\")\n",
    "if TEST_DIR:\n",
    "    print(f\"Test images:      {len([f for f in TEST_DIR.rglob('*') if f.suffix.lower() in img_exts])}\")\n",
    "\n",
    "# >>> MANUAL OVERRIDE: Uncomment and edit if auto-detect fails <<<\n",
    "# DIST_DIR = INPUT_DIR / 'train' / 'distorted'\n",
    "# CORR_DIR = INPUT_DIR / 'train' / 'corrected'\n",
    "# TEST_DIR = INPUT_DIR / 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — Visualize Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_match(stem, directory):\n",
    "    for ext in img_exts:\n",
    "        p = directory / (stem + ext)\n",
    "        if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "def show_pairs(n=4):\n",
    "    dist_files = sorted([f for f in DIST_DIR.iterdir() if f.suffix.lower() in img_exts])[:n]\n",
    "    fig, axes = plt.subplots(n, 2, figsize=(12, 4*n))\n",
    "    if n == 1: axes = [axes]\n",
    "    for i, df in enumerate(dist_files):\n",
    "        cf = find_match(df.stem, CORR_DIR)\n",
    "        d_img = cv2.cvtColor(cv2.imread(str(df)), cv2.COLOR_BGR2RGB)\n",
    "        c_img = cv2.cvtColor(cv2.imread(str(cf)), cv2.COLOR_BGR2RGB)\n",
    "        axes[i][0].imshow(d_img); axes[i][0].set_title(f'Distorted: {df.name}'); axes[i][0].axis('off')\n",
    "        axes[i][1].imshow(c_img); axes[i][1].set_title(f'Corrected: {cf.name}'); axes[i][1].axis('off')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "if DIST_DIR and CORR_DIR:\n",
    "    show_pairs(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — Extract Distortion Parameters from Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PARAMS_CSV = WORK_DIR / 'params.csv'\n",
    "\n",
    "def undistort_image(img, k1, k2, k3, cx, cy):\n",
    "    h, w = img.shape[:2]\n",
    "    fx = fy = max(h, w)\n",
    "    cam = np.array([[fx, 0, cx*w], [0, fy, cy*h], [0, 0, 1]], dtype=np.float64)\n",
    "    dist = np.array([k1, k2, 0, 0, k3], dtype=np.float64)\n",
    "    new_cam, roi = cv2.getOptimalNewCameraMatrix(cam, dist, (w, h), alpha=0)\n",
    "    out = cv2.undistort(img, cam, dist, None, new_cam)\n",
    "    x, y, rw, rh = roi\n",
    "    if rw > 0 and rh > 0:\n",
    "        out = cv2.resize(out[y:y+rh, x:x+rw], (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    return out\n",
    "\n",
    "def objective(params, distorted, corrected):\n",
    "    try:\n",
    "        u = undistort_image(distorted, *params)\n",
    "        return np.mean((u.astype(np.float32) - corrected.astype(np.float32))**2)\n",
    "    except Exception:\n",
    "        return 1e10\n",
    "\n",
    "def extract_single(args):\n",
    "    dp, cp, sz = args\n",
    "    d = cv2.imread(str(dp))\n",
    "    c = cv2.imread(str(cp))\n",
    "    if d is None or c is None: return None\n",
    "    d = cv2.resize(d, (sz, sz), interpolation=cv2.INTER_AREA)\n",
    "    c = cv2.resize(c, (sz, sz), interpolation=cv2.INTER_AREA)\n",
    "    res = minimize(objective, [0,0,0,0.5,0.5], args=(d,c), method='L-BFGS-B',\n",
    "                   bounds=[(-1,1),(-1,1),(-1,1),(0.3,0.7),(0.3,0.7)],\n",
    "                   options={'maxiter': 200, 'ftol': 1e-8})\n",
    "    return res.x\n",
    "\n",
    "# Match pairs\n",
    "dist_files = sorted([f for f in DIST_DIR.iterdir() if f.suffix.lower() in img_exts])\n",
    "pairs = []\n",
    "for df in dist_files:\n",
    "    cf = find_match(df.stem, CORR_DIR)\n",
    "    if cf: pairs.append((df, cf))\n",
    "print(f\"Matched {len(pairs)} training pairs\")\n",
    "\n",
    "# Extract in parallel\n",
    "results = []\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    futures = {executor.submit(extract_single, (d,c,256)): d.stem for d,c in pairs}\n",
    "    for f in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting params\"):\n",
    "        img_id = futures[f]\n",
    "        try:\n",
    "            p = f.result()\n",
    "            if p is not None: results.append((img_id, *p))\n",
    "        except Exception as e:\n",
    "            print(f\"Error {img_id}: {e}\")\n",
    "\n",
    "with open(PARAMS_CSV, 'w', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['image_id','k1','k2','k3','cx','cy'])\n",
    "    for row in results: w.writerow(row)\n",
    "print(f\"\\nSaved {len(results)} params to {PARAMS_CSV}\")\n",
    "\n",
    "# Validate\n",
    "print(\"\\nPSNR validation:\")\n",
    "pdict = {r[0]: r[1:] for r in results}\n",
    "psnrs = []\n",
    "for dp, cp in pairs[:5]:\n",
    "    if dp.stem not in pdict: continue\n",
    "    d, c = cv2.imread(str(dp)), cv2.imread(str(cp))\n",
    "    u = undistort_image(d, *pdict[dp.stem])\n",
    "    mse = np.mean((u.astype(np.float32) - c.astype(np.float32))**2)\n",
    "    psnr = 10*np.log10(255**2/max(mse,1e-10))\n",
    "    psnrs.append(psnr)\n",
    "    print(f\"  {dp.stem}: {psnr:.2f} dB\")\n",
    "if psnrs: print(f\"  Average: {np.mean(psnrs):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — Dataset & Model Definition\n",
    "\n",
    "Uses `torchvision.models.efficientnet_b3` (pre-cached on Kaggle, no download needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "class DistortionDataset(Dataset):\n",
    "    def __init__(self, image_dir, params_csv, image_size=224, augment=True, corrected_dir=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.corrected_dir = Path(corrected_dir) if corrected_dir else None\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "        self.samples = []\n",
    "        with open(params_csv, 'r') as f:\n",
    "            for row in csv.DictReader(f):\n",
    "                p = np.array([float(row[k]) for k in ['k1','k2','k3','cx','cy']], dtype=np.float32)\n",
    "                self.samples.append((row['image_id'], p))\n",
    "        self._paths = {}\n",
    "        for f in self.image_dir.iterdir():\n",
    "            if f.suffix.lower() in img_exts: self._paths[f.stem] = f\n",
    "        self.samples = [(i,p) for i,p in self.samples if i in self._paths]\n",
    "        self._build_transforms()\n",
    "\n",
    "    def _build_transforms(self):\n",
    "        sz = self.image_size\n",
    "        if self.augment:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((sz, sz)),\n",
    "                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])\n",
    "        else:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((sz, sz)), T.ToTensor(),\n",
    "                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])\n",
    "        self.target_transform = T.Compose([\n",
    "            T.Resize((sz, sz)), T.ToTensor(),\n",
    "            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])\n",
    "\n",
    "    def update_image_size(self, new_size):\n",
    "        self.image_size = new_size\n",
    "        self._build_transforms()\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, params = self.samples[idx]\n",
    "        img = Image.open(self._paths[image_id]).convert('RGB')\n",
    "        result = {'image': self.transform(img), 'params': torch.from_numpy(params), 'image_id': image_id}\n",
    "        if self.corrected_dir is not None:\n",
    "            cp = find_match(image_id, self.corrected_dir)\n",
    "            if cp:\n",
    "                result['corrected'] = self.target_transform(Image.open(cp).convert('RGB'))\n",
    "        return result\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_size=384):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.files = sorted([f for f in self.image_dir.iterdir() if f.suffix.lower() in img_exts])\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)), T.ToTensor(),\n",
    "            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.files[idx]\n",
    "        img = Image.open(p).convert('RGB')\n",
    "        w, h = img.size\n",
    "        return {'image': self.transform(img), 'image_id': p.stem,\n",
    "                'image_path': str(p), 'orig_h': h, 'orig_w': w}\n",
    "\n",
    "\n",
    "class DistortionNet(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        weights = models.EfficientNet_B3_Weights.DEFAULT if pretrained else None\n",
    "        backbone = models.efficientnet_b3(weights=weights)\n",
    "        feat_dim = backbone.classifier[1].in_features  # 1536\n",
    "        backbone.classifier = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2), nn.Linear(256, 5))\n",
    "        nn.init.zeros_(self.head[-1].weight)\n",
    "        nn.init.zeros_(self.head[-1].bias)\n",
    "        with torch.no_grad():\n",
    "            self.head[-1].bias[3] = 0.5\n",
    "            self.head[-1].bias[4] = 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        p = self.head(feat)\n",
    "        return torch.cat([torch.tanh(p[:,:3]), torch.sigmoid(p[:,3:])], dim=1)\n",
    "\n",
    "\n",
    "def differentiable_undistort(image, params):\n",
    "    B, C, H, W = image.shape\n",
    "    k1, k2, k3 = params[:,0:1], params[:,1:2], params[:,2:3]\n",
    "    cx, cy = params[:,3:4], params[:,4:5]\n",
    "    gy, gx = torch.meshgrid(\n",
    "        torch.linspace(-1,1,H,device=image.device),\n",
    "        torch.linspace(-1,1,W,device=image.device), indexing='ij')\n",
    "    gx = gx.unsqueeze(0).expand(B,-1,-1)\n",
    "    gy = gy.unsqueeze(0).expand(B,-1,-1)\n",
    "    cx_n = (cx*2-1).unsqueeze(-1)\n",
    "    cy_n = (cy*2-1).unsqueeze(-1)\n",
    "    dx, dy = gx - cx_n, gy - cy_n\n",
    "    r2 = dx**2 + dy**2\n",
    "    k1, k2, k3 = k1.unsqueeze(-1), k2.unsqueeze(-1), k3.unsqueeze(-1)\n",
    "    radial = 1 + k1*r2 + k2*r2**2 + k3*r2**3\n",
    "    grid = torch.stack([dx*radial + cx_n, dy*radial + cy_n], dim=-1)\n",
    "    return F.grid_sample(image, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "\n",
    "class DistortionLoss(nn.Module):\n",
    "    def __init__(self, param_weight=1.0, pixel_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.pw, self.xw = param_weight, pixel_weight\n",
    "\n",
    "    def forward(self, pred, gt, dist_img=None, corr_img=None):\n",
    "        lp = F.mse_loss(pred, gt)\n",
    "        lx = torch.tensor(0.0, device=pred.device)\n",
    "        if dist_img is not None and corr_img is not None:\n",
    "            lx = F.l1_loss(differentiable_undistort(dist_img, pred), corr_img)\n",
    "        return self.pw*lp + self.xw*lx, lp, lx\n",
    "\n",
    "\n",
    "print(\"Classes defined.\")\n",
    "model = DistortionNet(pretrained=True).to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "LR = 3e-3\n",
    "NUM_WORKERS = 2\n",
    "CKPT_DIR = WORK_DIR / 'checkpoints'\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "SIZE_SCHEDULE = {0: 224, EPOCHS//3: 384, 2*EPOCHS//3: 512}\n",
    "\n",
    "full_ds = DistortionDataset(DIST_DIR, PARAMS_CSV, image_size=224, augment=True, corrected_dir=CORR_DIR)\n",
    "n_val = int(len(full_ds) * 0.2)\n",
    "n_train = len(full_ds) - n_val\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "val_ds_noaug = DistortionDataset(DIST_DIR, PARAMS_CSV, image_size=224, augment=False, corrected_dir=CORR_DIR)\n",
    "val_ds_noaug.samples = [val_ds_noaug.samples[i] for i in val_ds.indices]\n",
    "print(f\"Train: {n_train}, Val: {n_val}\")\n",
    "\n",
    "model = DistortionNet(pretrained=True).to(DEVICE)\n",
    "criterion = DistortionLoss(param_weight=1.0, pixel_weight=0.5)\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=LR, total_steps=(n_train//BATCH_SIZE+1)*EPOCHS,\n",
    "                       pct_start=0.3, div_factor=25, final_div_factor=1000)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    if epoch-1 in SIZE_SCHEDULE:\n",
    "        sz = SIZE_SCHEDULE[epoch-1]\n",
    "        print(f\"\\n>>> Resize to {sz}x{sz}\")\n",
    "        full_ds.update_image_size(sz)\n",
    "        val_ds_noaug.update_image_size(sz)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds_noaug, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    model.train()\n",
    "    t_loss, t_pl, t_xl, nb = 0, 0, 0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "    for batch in pbar:\n",
    "        imgs = batch['image'].to(DEVICE)\n",
    "        params = batch['params'].to(DEVICE)\n",
    "        corr = batch.get('corrected')\n",
    "        corr = corr.to(DEVICE) if corr is not None else None\n",
    "        pred = model(imgs)\n",
    "        loss, lp, lx = criterion(pred, params, imgs, corr)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        t_loss += loss.item(); t_pl += lp.item(); t_xl += lx.item(); nb += 1\n",
    "        pbar.set_postfix(loss=f\"{t_loss/nb:.4f}\", param=f\"{t_pl/nb:.4f}\", pixel=f\"{t_xl/nb:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    v_loss, errs = 0, []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            imgs = batch['image'].to(DEVICE)\n",
    "            params = batch['params'].to(DEVICE)\n",
    "            corr = batch.get('corrected')\n",
    "            corr = corr.to(DEVICE) if corr is not None else None\n",
    "            pred = model(imgs)\n",
    "            loss, lp, lx = criterion(pred, params, imgs, corr)\n",
    "            v_loss += loss.item()\n",
    "            errs.append((pred - params).abs().cpu().numpy())\n",
    "    n_vb = max(len(val_loader), 1)\n",
    "    v_loss /= n_vb\n",
    "    me = np.concatenate(errs).mean(axis=0) if errs else np.zeros(5)\n",
    "    print(f\"Epoch {epoch}: train={t_loss/nb:.4f} val={v_loss:.4f} | \"\n",
    "          f\"k1e={me[0]:.4f} k2e={me[1]:.4f} k3e={me[2]:.4f} cxe={me[3]:.4f} cye={me[4]:.4f}\")\n",
    "    history.append({'epoch': epoch, 'train_loss': t_loss/nb, 'val_loss': v_loss})\n",
    "\n",
    "    if v_loss < best_val_loss:\n",
    "        best_val_loss = v_loss\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "                    'val_loss': best_val_loss}, CKPT_DIR / 'best_model.pth')\n",
    "        print(f\"  >> Saved best (val_loss={best_val_loss:.4f})\")\n",
    "\n",
    "print(f\"\\nDone. Best val_loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([h['epoch'] for h in history], [h['train_loss'] for h in history], label='Train')\n",
    "ax.plot([h['epoch'] for h in history], [h['val_loss'] for h in history], label='Val')\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.legend(); ax.set_title('Training Curves')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 — Local Scoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def edge_similarity(img1, img2, scales=(1.0, 0.5, 0.25)):\n",
    "    g1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) if len(img1.shape)==3 else img1\n",
    "    g2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) if len(img2.shape)==3 else img2\n",
    "    f1s = []\n",
    "    for s in scales:\n",
    "        a = cv2.resize(g1, (int(g1.shape[1]*s), int(g1.shape[0]*s))) if s != 1.0 else g1\n",
    "        b = cv2.resize(g2, (int(g2.shape[1]*s), int(g2.shape[0]*s))) if s != 1.0 else g2\n",
    "        e1 = cv2.Canny(a, int(max(0,0.67*np.median(a))), int(min(255,1.33*np.median(a))))\n",
    "        e2 = cv2.Canny(b, int(max(0,0.67*np.median(b))), int(min(255,1.33*np.median(b))))\n",
    "        k = np.ones((3,3), np.uint8)\n",
    "        e1d, e2d = cv2.dilate(e1,k,iterations=1), cv2.dilate(e2,k,iterations=1)\n",
    "        if e1.sum()==0 and e2.sum()==0: f1s.append(1.0); continue\n",
    "        if e1.sum()==0 or e2.sum()==0: f1s.append(0.0); continue\n",
    "        p = (e1 & e2d).sum()/max(e1.sum(),1)\n",
    "        r = (e2 & e1d).sum()/max(e2.sum(),1)\n",
    "        f1s.append(2*p*r/(p+r) if p+r>0 else 0.0)\n",
    "    return np.mean(f1s)\n",
    "\n",
    "def line_straightness(img, ref=None):\n",
    "    g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape)==3 else img\n",
    "    lines = cv2.HoughLinesP(cv2.Canny(g,50,150,apertureSize=3), 1, np.pi/180, 50, minLineLength=30, maxLineGap=10)\n",
    "    if lines is None: return 0.5\n",
    "    angles = np.array([np.arctan2(l[0][3]-l[0][1], l[0][2]-l[0][0])*180/np.pi for l in lines])\n",
    "    if ref is not None:\n",
    "        gr = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY) if len(ref.shape)==3 else ref\n",
    "        lr = cv2.HoughLinesP(cv2.Canny(gr,50,150,apertureSize=3), 1, np.pi/180, 50, minLineLength=30, maxLineGap=10)\n",
    "        if lr is None: return 0.5\n",
    "        ar = np.array([np.arctan2(l[0][3]-l[0][1], l[0][2]-l[0][0])*180/np.pi for l in lr])\n",
    "        bins = np.linspace(-90,90,37)\n",
    "        h1,_ = np.histogram(angles,bins=bins,density=True)\n",
    "        h2,_ = np.histogram(ar,bins=bins,density=True)\n",
    "        h1, h2 = h1/(h1.sum()+1e-10), h2/(h2.sum()+1e-10)\n",
    "        return float(np.sum(np.sqrt(h1*h2)))\n",
    "    return float(np.mean(np.minimum(np.abs(angles), np.abs(np.abs(angles)-90))<5))\n",
    "\n",
    "def gradient_orientation_sim(img1, img2, n_bins=36):\n",
    "    g1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY).astype(np.float32) if len(img1.shape)==3 else img1.astype(np.float32)\n",
    "    g2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY).astype(np.float32) if len(img2.shape)==3 else img2.astype(np.float32)\n",
    "    gx1, gy1 = cv2.Sobel(g1,cv2.CV_32F,1,0,ksize=3), cv2.Sobel(g1,cv2.CV_32F,0,1,ksize=3)\n",
    "    gx2, gy2 = cv2.Sobel(g2,cv2.CV_32F,1,0,ksize=3), cv2.Sobel(g2,cv2.CV_32F,0,1,ksize=3)\n",
    "    bins = np.linspace(-np.pi, np.pi, n_bins+1)\n",
    "    h1,_ = np.histogram(np.arctan2(gy1,gx1), bins=bins, weights=np.sqrt(gx1**2+gy1**2))\n",
    "    h2,_ = np.histogram(np.arctan2(gy2,gx2), bins=bins, weights=np.sqrt(gx2**2+gy2**2))\n",
    "    h1, h2 = h1/(h1.sum()+1e-10), h2/(h2.sum()+1e-10)\n",
    "    return float(np.sum(np.sqrt(h1*h2)))\n",
    "\n",
    "def pixel_accuracy(img1, img2):\n",
    "    if img1.shape != img2.shape: img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))\n",
    "    return 1.0 - np.mean(np.abs(img1.astype(np.float32) - img2.astype(np.float32)))/255.0\n",
    "\n",
    "def compute_score(corrected, gt):\n",
    "    if corrected.shape != gt.shape: corrected = cv2.resize(corrected, (gt.shape[1], gt.shape[0]))\n",
    "    es = edge_similarity(corrected, gt)\n",
    "    ls = line_straightness(corrected, gt)\n",
    "    go = gradient_orientation_sim(corrected, gt)\n",
    "    ss = ssim(corrected, gt, channel_axis=2, data_range=255) if len(corrected.shape)==3 else ssim(corrected, gt, data_range=255)\n",
    "    pa = pixel_accuracy(corrected, gt)\n",
    "    overall = 0.40*es + 0.22*ls + 0.18*go + 0.15*ss + 0.05*pa\n",
    "    return overall, dict(edge=es, line=ls, grad=go, ssim=ss, pixel=pa, overall=overall)\n",
    "\n",
    "print(\"Metrics defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9 — Validate on Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(CKPT_DIR / 'best_model.pth', map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "print(f\"Loaded best model from epoch {ckpt['epoch']}\")\n",
    "\n",
    "N_VIS = min(8, len(val_ds_noaug))\n",
    "val_vis_loader = DataLoader(val_ds_noaug, batch_size=1, shuffle=False)\n",
    "\n",
    "scores = []\n",
    "fig, axes = plt.subplots(N_VIS, 3, figsize=(15, 4*N_VIS))\n",
    "if N_VIS == 1: axes = [axes]\n",
    "\n",
    "for i, batch in enumerate(val_vis_loader):\n",
    "    if i >= N_VIS: break\n",
    "    image_id = batch['image_id'][0]\n",
    "    with torch.no_grad():\n",
    "        pp = model(batch['image'].to(DEVICE)).cpu().numpy()[0]\n",
    "    dist_img = cv2.imread(str(next(DIST_DIR.glob(f\"{image_id}.*\"))))\n",
    "    corr_img = cv2.imread(str(next(CORR_DIR.glob(f\"{image_id}.*\"))))\n",
    "    pred_corr = undistort_image(dist_img, *pp)\n",
    "    overall, m = compute_score(pred_corr, corr_img)\n",
    "    scores.append(overall)\n",
    "    axes[i][0].imshow(cv2.cvtColor(dist_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[i][0].set_title('Distorted'); axes[i][0].axis('off')\n",
    "    axes[i][1].imshow(cv2.cvtColor(pred_corr, cv2.COLOR_BGR2RGB))\n",
    "    axes[i][1].set_title(f'Ours (score={overall:.3f})'); axes[i][1].axis('off')\n",
    "    axes[i][2].imshow(cv2.cvtColor(corr_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[i][2].set_title('Ground truth'); axes[i][2].axis('off')\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "print(f\"\\nAvg local score: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10 — Predict Test + Test-Time Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = WORK_DIR / 'output'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "ckpt = torch.load(CKPT_DIR / 'best_model.pth', map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "test_ds = TestDataset(TEST_DIR, image_size=384)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
    "print(f\"Test images: {len(test_ds)}\")\n",
    "\n",
    "# Stage 1: CNN predictions\n",
    "predictions = {}\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"CNN prediction\"):\n",
    "        preds = model(batch['image'].to(DEVICE))\n",
    "        for i, img_id in enumerate(batch['image_id']):\n",
    "            predictions[img_id] = {\n",
    "                'params': preds[i].cpu().numpy(),\n",
    "                'image_path': batch['image_path'][i],\n",
    "                'orig_h': batch['orig_h'][i].item(),\n",
    "                'orig_w': batch['orig_w'][i].item()}\n",
    "\n",
    "# Stage 2: Test-Time Optimization\n",
    "TTO_STEPS = 50\n",
    "\n",
    "def tto_loss(undistorted):\n",
    "    img = undistorted.squeeze(0)\n",
    "    gray = (0.299*img[0]+0.587*img[1]+0.114*img[2]).unsqueeze(0).unsqueeze(0)\n",
    "    sx = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]],dtype=torch.float32,device=img.device).view(1,1,3,3)\n",
    "    sy = torch.tensor([[-1,-2,-1],[0,0,0],[1,2,1]],dtype=torch.float32,device=img.device).view(1,1,3,3)\n",
    "    gx, gy = F.conv2d(gray,sx,padding=1), F.conv2d(gray,sy,padding=1)\n",
    "    edge_loss = -torch.sqrt(gx**2+gy**2+1e-6).mean()\n",
    "    C,H,W = img.shape\n",
    "    bs = max(2, min(H,W)//20)\n",
    "    borders = torch.cat([img[:,:bs,:].reshape(-1), img[:,-bs:,:].reshape(-1),\n",
    "                         img[:,:,:bs].reshape(-1), img[:,:,-bs:].reshape(-1)])\n",
    "    return 0.5*edge_loss + 0.5*(1.0-borders.abs().clamp(0,1)).mean()\n",
    "\n",
    "def diff_undistort(image, params):\n",
    "    B,C,H,W = image.shape\n",
    "    k1,k2,k3,cx,cy = params[0,0],params[0,1],params[0,2],params[0,3],params[0,4]\n",
    "    gy,gx = torch.meshgrid(torch.linspace(-1,1,H,device=image.device),\n",
    "                           torch.linspace(-1,1,W,device=image.device),indexing='ij')\n",
    "    dx, dy = gx-(cx*2-1), gy-(cy*2-1)\n",
    "    r2 = dx**2+dy**2\n",
    "    rad = 1+k1*r2+k2*r2**2+k3*r2**3\n",
    "    grid = torch.stack([dx*rad+(cx*2-1), dy*rad+(cy*2-1)], dim=-1).unsqueeze(0)\n",
    "    return F.grid_sample(image, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "print(f\"\\nRunning TTO ({TTO_STEPS} steps/image)...\")\n",
    "for img_id in tqdm(predictions, desc=\"TTO\"):\n",
    "    pred = predictions[img_id]\n",
    "    img = cv2.cvtColor(cv2.imread(pred['image_path']), cv2.COLOR_BGR2RGB)\n",
    "    img_t = torch.from_numpy(cv2.resize(img,(256,256))).float().permute(2,0,1).unsqueeze(0).to(DEVICE)/255.0\n",
    "    p = torch.tensor(pred['params'],dtype=torch.float32,device=DEVICE).unsqueeze(0).clone().detach().requires_grad_(True)\n",
    "    init_p = torch.tensor(pred['params'],dtype=torch.float32,device=DEVICE)\n",
    "    opt = torch.optim.Adam([p], lr=0.001)\n",
    "    best_loss, best_p = float('inf'), pred['params'].copy()\n",
    "    for _ in range(TTO_STEPS):\n",
    "        opt.zero_grad()\n",
    "        with torch.no_grad(): p.data[:,:3].clamp_(-1,1); p.data[:,3:].clamp_(0.1,0.9)\n",
    "        loss = tto_loss(diff_undistort(img_t, p)) + 0.1*F.mse_loss(p.squeeze(), init_p)\n",
    "        loss.backward(); opt.step()\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_p = p.detach().squeeze().cpu().numpy().copy()\n",
    "    pred['params_tto'] = best_p\n",
    "\n",
    "print(\"TTO complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 — Apply Corrections & Create Submission ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for img_id, pred in tqdm(predictions.items(), desc=\"Saving corrected\"):\n",
    "    img = cv2.imread(pred['image_path'])\n",
    "    if img is None: continue\n",
    "    corrected = undistort_image(img, *pred.get('params_tto', pred['params']))\n",
    "    cv2.imwrite(str(OUTPUT_DIR / f\"{img_id}.jpg\"), corrected, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
    "\n",
    "ZIP_PATH = WORK_DIR / 'submission.zip'\n",
    "with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for f in sorted(OUTPUT_DIR.glob('*.jpg')): zf.write(f, f.name)\n",
    "\n",
    "n_out = len(list(OUTPUT_DIR.glob('*.jpg')))\n",
    "print(f\"\\nSaved {n_out} corrected images\")\n",
    "print(f\"ZIP: {ZIP_PATH} ({ZIP_PATH.stat().st_size/1024/1024:.1f} MB)\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Download submission.zip from the Output tab\")\n",
    "print(f\"  2. Upload to https://bounty.autohdr.com\")\n",
    "print(f\"  3. Download the scoring CSV\")\n",
    "print(f\"  4. Submit CSV to Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12 — (Optional) Visualize Test Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_files = sorted(OUTPUT_DIR.glob('*.jpg'))[:6]\n",
    "fig, axes = plt.subplots(len(test_files), 2, figsize=(12, 4*len(test_files)))\n",
    "if len(test_files) == 1: axes = [axes]\n",
    "for i, cf in enumerate(test_files):\n",
    "    img_id = cf.stem\n",
    "    orig = cv2.cvtColor(cv2.imread(predictions[img_id]['image_path']), cv2.COLOR_BGR2RGB)\n",
    "    corr = cv2.cvtColor(cv2.imread(str(cf)), cv2.COLOR_BGR2RGB)\n",
    "    axes[i][0].imshow(orig); axes[i][0].set_title(f'Distorted: {img_id}'); axes[i][0].axis('off')\n",
    "    axes[i][1].imshow(corr); axes[i][1].set_title('Corrected'); axes[i][1].axis('off')\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
