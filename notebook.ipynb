{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Automatic Lens Correction\n\n**Approach:** Parametric distortion prediction (k1, k2, k3, cx, cy) using EfficientNet-B3 + differentiable undistortion + test-time optimization.\n\n**Works on:** Kaggle (offline), Google Colab, Paperspace, Lambda, any GPU box. Run Cell 0 first — it auto-detects your environment and sets everything up."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 0 — Environment Setup (run this first)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "import os, subprocess, sys\nfrom pathlib import Path\n\n# ── Detect environment ──\nENV = \"unknown\"\nif os.path.exists(\"/kaggle/input\"):\n    ENV = \"kaggle\"\nelif \"COLAB_RELEASE_TAG\" in os.environ or os.path.exists(\"/content\"):\n    ENV = \"colab\"\nelif os.path.exists(\"/notebooks\"):  # Paperspace Gradient\n    ENV = \"paperspace\"\nelse:\n    ENV = \"generic\"\n\nprint(f\"Detected environment: {ENV}\")\n\n# ── Install missing deps (Kaggle has everything, others may not) ──\nif ENV != \"kaggle\":\n    print(\"Installing dependencies...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n        \"torch\", \"torchvision\", \"opencv-python-headless\", \"scipy\",\n        \"scikit-image\", \"tqdm\", \"pandas\", \"Pillow\", \"matplotlib\", \"kaggle\"])\n\n# ── Set up paths ──\nif ENV == \"kaggle\":\n    INPUT_DIR = Path(\"/kaggle/input/automatic-lens-correction\")\n    WORK_DIR = Path(\"/kaggle/working\")\nelif ENV == \"colab\":\n    WORK_DIR = Path(\"/content/autohdr\")\n    INPUT_DIR = WORK_DIR / \"data\"\nelif ENV == \"paperspace\":\n    WORK_DIR = Path(\"/notebooks/autohdr\")\n    INPUT_DIR = WORK_DIR / \"data\"\nelse:\n    WORK_DIR = Path(\".\")\n    INPUT_DIR = WORK_DIR / \"data\"\n\nWORK_DIR.mkdir(parents=True, exist_ok=True)\nINPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ── Download data if not on Kaggle ──\nif ENV != \"kaggle\":\n    marker = INPUT_DIR / \".downloaded\"\n    if not marker.exists():\n        print(\"\\nDownloading competition data...\")\n        print(\"Make sure ~/.kaggle/kaggle.json exists with your API credentials.\")\n        if ENV == \"colab\":\n            # Colab: prompt for upload if missing\n            kaggle_json = Path.home() / \".kaggle\" / \"kaggle.json\"\n            if not kaggle_json.exists():\n                print(\"Upload your kaggle.json:\")\n                from google.colab import files\n                uploaded = files.upload()\n                kaggle_json.parent.mkdir(exist_ok=True)\n                for fname, content in uploaded.items():\n                    kaggle_json.write_bytes(content)\n                kaggle_json.chmod(0o600)\n\n        subprocess.check_call([\"kaggle\", \"competitions\", \"download\",\n                               \"-c\", \"automatic-lens-correction\",\n                               \"-p\", str(INPUT_DIR)])\n        # Unzip all zip files\n        import zipfile\n        for zf in INPUT_DIR.glob(\"*.zip\"):\n            print(f\"Extracting {zf.name}...\")\n            with zipfile.ZipFile(zf, 'r') as z:\n                z.extractall(INPUT_DIR)\n            zf.unlink()\n        marker.touch()\n        print(\"Data ready.\")\n    else:\n        print(\"Data already downloaded.\")\n\nprint(f\"\\nINPUT_DIR: {INPUT_DIR}\")\nprint(f\"WORK_DIR:  {WORK_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 1 — Imports & Dependency Fix",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Fix numpy/skimage compatibility issue\nimport subprocess\nimport sys\n\ntry:\n    from skimage.metrics import structural_similarity as ssim\nexcept ValueError as e:\n    print(f\"Skimage import error: {e}\")\n    print(\"Installing compatible versions...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"numpy<2.0\", \"scikit-image\"])\n    from skimage.metrics import structural_similarity as ssim\n\nimport csv\nimport zipfile\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom scipy.optimize import minimize\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision import models, transforms as T\nfrom tqdm import tqdm\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {DEVICE}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2 — Discover Data Layout\n\nThe data uses `_original.jpg` / `_generated.jpg` pairs in the same folder."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "img_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n\ndef discover_data(data_dir):\n    \"\"\"Find train pairs (_original/_generated in same dir) and test dir.\"\"\"\n    data_dir = Path(data_dir)\n    train_pairs = []  # list of (original_path, generated_path, image_id)\n    test_files = []   # list of (path, image_id)\n    train_dir = None\n    test_dir = None\n\n    # Debug: show top-level structure\n    print(\"Data directory contents:\")\n    for p in sorted(data_dir.iterdir()):\n        kind = \"dir\" if p.is_dir() else \"file\"\n        print(f\"  [{kind}] {p.name}\")\n\n    # Walk all subdirectories looking for _original / _generated pairs\n    for d in sorted(data_dir.rglob('*')):\n        if not d.is_dir():\n            continue\n        all_images = sorted([f for f in d.iterdir() if f.suffix.lower() in img_exts])\n        if not all_images:\n            continue\n\n        originals = [f for f in all_images if '_original' in f.stem]\n        generated = [f for f in all_images if '_generated' in f.stem]\n\n        if originals and generated:\n            # Training directory — build pairs\n            gen_map = {}\n            for g in generated:\n                key = g.stem.rsplit('_generated', 1)[0]\n                gen_map[key] = g\n            for o in originals:\n                key = o.stem.rsplit('_original', 1)[0]\n                if key in gen_map:\n                    train_pairs.append((o, gen_map[key], key))\n            if train_pairs:\n                train_dir = d\n\n        elif originals and not generated:\n            # Directory with only _original files (no _generated) = test\n            for o in originals:\n                key = o.stem.rsplit('_original', 1)[0]\n                test_files.append((o, key))\n            test_dir = d\n\n    # Fallback for test: look for dirs with \"test\" in the name containing\n    # plain image files (no _original/_generated suffix)\n    if not test_files:\n        for d in sorted(data_dir.rglob('*')):\n            if not d.is_dir():\n                continue\n            if 'test' not in d.name.lower():\n                continue\n            all_images = sorted([f for f in d.iterdir() if f.suffix.lower() in img_exts])\n            if all_images:\n                for f in all_images:\n                    test_files.append((f, f.stem))\n                test_dir = d\n                print(f\"  Found test dir (plain filenames): {d}\")\n                break\n\n    return train_pairs, train_dir, test_files, test_dir\n\nTRAIN_PAIRS, TRAIN_DIR, TEST_FILES, TEST_DIR = discover_data(INPUT_DIR)\n\nprint(f\"\\nTrain directory: {TRAIN_DIR}\")\nprint(f\"Train pairs:     {len(TRAIN_PAIRS)}\")\nif TRAIN_PAIRS:\n    o, g, key = TRAIN_PAIRS[0]\n    print(f\"  Example: {o.name}  <->  {g.name}  (id: {key})\")\n\nprint(f\"\\nTest directory:  {TEST_DIR}\")\nprint(f\"Test images:     {len(TEST_FILES)}\")\nif TEST_FILES:\n    print(f\"  Example: {TEST_FILES[0][0].name}  (id: {TEST_FILES[0][1]})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — Visualize Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "def show_pairs(n=4):\n    fig, axes = plt.subplots(n, 2, figsize=(12, 4*n))\n    if n == 1: axes = [axes]\n    for i in range(min(n, len(TRAIN_PAIRS))):\n        orig_path, gen_path, img_id = TRAIN_PAIRS[i]\n        d_img = cv2.cvtColor(cv2.imread(str(orig_path)), cv2.COLOR_BGR2RGB)\n        c_img = cv2.cvtColor(cv2.imread(str(gen_path)), cv2.COLOR_BGR2RGB)\n        axes[i][0].imshow(d_img); axes[i][0].set_title(f'Original (distorted): {img_id}'); axes[i][0].axis('off')\n        axes[i][1].imshow(c_img); axes[i][1].set_title(f'Generated (corrected)'); axes[i][1].axis('off')\n    plt.tight_layout(); plt.show()\n\nif TRAIN_PAIRS:\n    show_pairs(4)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — Extract Distortion Parameters from Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "PARAMS_CSV = WORK_DIR / 'params.csv'\n\ndef undistort_image(img, k1, k2, k3, cx, cy):\n    h, w = img.shape[:2]\n    fx = fy = max(h, w)\n    cam = np.array([[fx, 0, cx*w], [0, fy, cy*h], [0, 0, 1]], dtype=np.float64)\n    dist = np.array([k1, k2, 0, 0, k3], dtype=np.float64)\n    new_cam, roi = cv2.getOptimalNewCameraMatrix(cam, dist, (w, h), alpha=0)\n    out = cv2.undistort(img, cam, dist, None, new_cam)\n    x, y, rw, rh = roi\n    if rw > 0 and rh > 0:\n        out = cv2.resize(out[y:y+rh, x:x+rw], (w, h), interpolation=cv2.INTER_LINEAR)\n    return out\n\ndef objective(params, distorted, corrected):\n    try:\n        u = undistort_image(distorted, *params)\n        return np.mean((u.astype(np.float32) - corrected.astype(np.float32))**2)\n    except Exception:\n        return 1e10\n\ndef extract_single(args):\n    dp, cp, sz = args\n    d = cv2.imread(str(dp))\n    c = cv2.imread(str(cp))\n    if d is None or c is None: return None\n    d = cv2.resize(d, (sz, sz), interpolation=cv2.INTER_AREA)\n    c = cv2.resize(c, (sz, sz), interpolation=cv2.INTER_AREA)\n    res = minimize(objective, [0,0,0,0.5,0.5], args=(d,c), method='L-BFGS-B',\n                   bounds=[(-1,1),(-1,1),(-1,1),(0.3,0.7),(0.3,0.7)],\n                   options={'maxiter': 200, 'ftol': 1e-8})\n    return res.x\n\nprint(f\"Extracting params from {len(TRAIN_PAIRS)} pairs...\")\n\nresults = []\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    futures = {}\n    for orig_path, gen_path, img_id in TRAIN_PAIRS:\n        f = executor.submit(extract_single, (orig_path, gen_path, 256))\n        futures[f] = img_id\n    for f in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting params\"):\n        img_id = futures[f]\n        try:\n            p = f.result()\n            if p is not None: results.append((img_id, *p))\n        except Exception as e:\n            print(f\"Error {img_id}: {e}\")\n\nwith open(PARAMS_CSV, 'w', newline='') as f:\n    w = csv.writer(f)\n    w.writerow(['image_id','k1','k2','k3','cx','cy'])\n    for row in results: w.writerow(row)\nprint(f\"\\nSaved {len(results)} params to {PARAMS_CSV}\")\n\n# Build lookup for original/generated paths by image_id\nPAIR_LOOKUP = {img_id: (op, gp) for op, gp, img_id in TRAIN_PAIRS}\n\n# Validate\nprint(\"\\nPSNR validation:\")\npdict = {r[0]: r[1:] for r in results}\npsnrs = []\nfor _, _, img_id in TRAIN_PAIRS[:5]:\n    if img_id not in pdict: continue\n    op, gp = PAIR_LOOKUP[img_id]\n    d, c = cv2.imread(str(op)), cv2.imread(str(gp))\n    u = undistort_image(d, *pdict[img_id])\n    mse = np.mean((u.astype(np.float32) - c.astype(np.float32))**2)\n    psnr = 10*np.log10(255**2/max(mse,1e-10))\n    psnrs.append(psnr)\n    print(f\"  {img_id}: {psnr:.2f} dB\")\nif psnrs: print(f\"  Average: {np.mean(psnrs):.2f} dB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — Dataset & Model Definition\n",
    "\n",
    "Uses `torchvision.models.efficientnet_b3` (pre-cached on Kaggle, no download needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "IMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\nclass DistortionDataset(Dataset):\n    \"\"\"Loads (distorted_image, params) pairs. Uses PAIR_LOOKUP for corrected images.\"\"\"\n    def __init__(self, pair_lookup, params_csv, image_size=224, augment=True):\n        self.pair_lookup = pair_lookup  # {image_id: (original_path, generated_path)}\n        self.image_size = image_size\n        self.augment = augment\n        self.samples = []\n        with open(params_csv, 'r') as f:\n            for row in csv.DictReader(f):\n                img_id = row['image_id']\n                if img_id in self.pair_lookup:\n                    p = np.array([float(row[k]) for k in ['k1','k2','k3','cx','cy']], dtype=np.float32)\n                    self.samples.append((img_id, p))\n        self._build_transforms()\n\n    def _build_transforms(self):\n        sz = self.image_size\n        if self.augment:\n            self.transform = T.Compose([\n                T.Resize((sz, sz)),\n                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n                T.ToTensor(),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])\n        else:\n            self.transform = T.Compose([\n                T.Resize((sz, sz)), T.ToTensor(),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])\n        self.target_transform = T.Compose([\n            T.Resize((sz, sz)), T.ToTensor(),\n            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])\n\n    def update_image_size(self, new_size):\n        self.image_size = new_size\n        self._build_transforms()\n\n    def __len__(self): return len(self.samples)\n\n    def __getitem__(self, idx):\n        image_id, params = self.samples[idx]\n        orig_path, gen_path = self.pair_lookup[image_id]\n        img = Image.open(orig_path).convert('RGB')\n        result = {'image': self.transform(img), 'params': torch.from_numpy(params), 'image_id': image_id}\n        if gen_path.exists():\n            result['corrected'] = self.target_transform(Image.open(gen_path).convert('RGB'))\n        return result\n\n\nclass TestDataset(Dataset):\n    \"\"\"Loads test images. Uses TEST_FILES list of (path, image_id).\"\"\"\n    def __init__(self, test_files, image_size=384):\n        self.test_files = test_files  # list of (path, image_id)\n        self.transform = T.Compose([\n            T.Resize((image_size, image_size)), T.ToTensor(),\n            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])\n\n    def __len__(self): return len(self.test_files)\n\n    def __getitem__(self, idx):\n        p, img_id = self.test_files[idx]\n        img = Image.open(p).convert('RGB')\n        w, h = img.size\n        return {'image': self.transform(img), 'image_id': img_id,\n                'image_path': str(p), 'orig_h': h, 'orig_w': w}\n\n\nclass DistortionNet(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        weights = models.EfficientNet_B3_Weights.DEFAULT if pretrained else None\n        backbone = models.efficientnet_b3(weights=weights)\n        feat_dim = backbone.classifier[1].in_features  # 1536\n        backbone.classifier = nn.Identity()\n        self.backbone = backbone\n        self.head = nn.Sequential(\n            nn.Linear(feat_dim, 256), nn.ReLU(inplace=True),\n            nn.Dropout(0.2), nn.Linear(256, 5))\n        nn.init.zeros_(self.head[-1].weight)\n        nn.init.zeros_(self.head[-1].bias)\n        with torch.no_grad():\n            self.head[-1].bias[3] = 0.5\n            self.head[-1].bias[4] = 0.5\n\n    def forward(self, x):\n        feat = self.backbone(x)\n        p = self.head(feat)\n        return torch.cat([torch.tanh(p[:,:3]), torch.sigmoid(p[:,3:])], dim=1)\n\n\ndef differentiable_undistort(image, params):\n    B, C, H, W = image.shape\n    k1, k2, k3 = params[:,0:1], params[:,1:2], params[:,2:3]\n    cx, cy = params[:,3:4], params[:,4:5]\n    gy, gx = torch.meshgrid(\n        torch.linspace(-1,1,H,device=image.device),\n        torch.linspace(-1,1,W,device=image.device), indexing='ij')\n    gx = gx.unsqueeze(0).expand(B,-1,-1)\n    gy = gy.unsqueeze(0).expand(B,-1,-1)\n    cx_n = (cx*2-1).unsqueeze(-1)\n    cy_n = (cy*2-1).unsqueeze(-1)\n    dx, dy = gx - cx_n, gy - cy_n\n    r2 = dx**2 + dy**2\n    k1, k2, k3 = k1.unsqueeze(-1), k2.unsqueeze(-1), k3.unsqueeze(-1)\n    radial = 1 + k1*r2 + k2*r2**2 + k3*r2**3\n    grid = torch.stack([dx*radial + cx_n, dy*radial + cy_n], dim=-1)\n    return F.grid_sample(image, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n\n\nclass DistortionLoss(nn.Module):\n    def __init__(self, param_weight=1.0, pixel_weight=0.5):\n        super().__init__()\n        self.pw, self.xw = param_weight, pixel_weight\n\n    def forward(self, pred, gt, dist_img=None, corr_img=None):\n        lp = F.mse_loss(pred, gt)\n        lx = torch.tensor(0.0, device=pred.device)\n        if dist_img is not None and corr_img is not None:\n            lx = F.l1_loss(differentiable_undistort(dist_img, pred), corr_img)\n        return self.pw*lp + self.xw*lx, lp, lx\n\n\nprint(\"Classes defined.\")\nmodel = DistortionNet(pretrained=True).to(DEVICE)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "EPOCHS = 30\nBATCH_SIZE = 16\nLR = 3e-3\nNUM_WORKERS = 2\nCKPT_DIR = WORK_DIR / 'checkpoints'\nCKPT_DIR.mkdir(exist_ok=True)\nSIZE_SCHEDULE = {0: 224, EPOCHS//3: 384, 2*EPOCHS//3: 512}\n\nfull_ds = DistortionDataset(PAIR_LOOKUP, PARAMS_CSV, image_size=224, augment=True)\nn_val = int(len(full_ds) * 0.2)\nn_train = len(full_ds) - n_val\ntrain_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=torch.Generator().manual_seed(42))\nval_ds_noaug = DistortionDataset(PAIR_LOOKUP, PARAMS_CSV, image_size=224, augment=False)\nval_ds_noaug.samples = [val_ds_noaug.samples[i] for i in val_ds.indices]\nprint(f\"Train: {n_train}, Val: {n_val}\")\n\nmodel = DistortionNet(pretrained=True).to(DEVICE)\ncriterion = DistortionLoss(param_weight=1.0, pixel_weight=0.5)\noptimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\nscheduler = OneCycleLR(optimizer, max_lr=LR, total_steps=(n_train//BATCH_SIZE+1)*EPOCHS,\n                       pct_start=0.3, div_factor=25, final_div_factor=1000)\n\nbest_val_loss = float('inf')\nhistory = []\n\nfor epoch in range(1, EPOCHS+1):\n    if epoch-1 in SIZE_SCHEDULE:\n        sz = SIZE_SCHEDULE[epoch-1]\n        print(f\"\\n>>> Resize to {sz}x{sz}\")\n        full_ds.update_image_size(sz)\n        val_ds_noaug.update_image_size(sz)\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                              num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_ds_noaug, batch_size=BATCH_SIZE, shuffle=False,\n                            num_workers=NUM_WORKERS, pin_memory=True)\n\n    model.train()\n    t_loss, t_pl, t_xl, nb = 0, 0, 0, 0\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n    for batch in pbar:\n        imgs = batch['image'].to(DEVICE)\n        params = batch['params'].to(DEVICE)\n        corr = batch.get('corrected')\n        corr = corr.to(DEVICE) if corr is not None else None\n        pred = model(imgs)\n        loss, lp, lx = criterion(pred, params, imgs, corr)\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        t_loss += loss.item(); t_pl += lp.item(); t_xl += lx.item(); nb += 1\n        pbar.set_postfix(loss=f\"{t_loss/nb:.4f}\", param=f\"{t_pl/nb:.4f}\", pixel=f\"{t_xl/nb:.4f}\")\n\n    model.eval()\n    v_loss, errs = 0, []\n    with torch.no_grad():\n        for batch in val_loader:\n            imgs = batch['image'].to(DEVICE)\n            params = batch['params'].to(DEVICE)\n            corr = batch.get('corrected')\n            corr = corr.to(DEVICE) if corr is not None else None\n            pred = model(imgs)\n            loss, lp, lx = criterion(pred, params, imgs, corr)\n            v_loss += loss.item()\n            errs.append((pred - params).abs().cpu().numpy())\n    n_vb = max(len(val_loader), 1)\n    v_loss /= n_vb\n    me = np.concatenate(errs).mean(axis=0) if errs else np.zeros(5)\n    print(f\"Epoch {epoch}: train={t_loss/nb:.4f} val={v_loss:.4f} | \"\n          f\"k1e={me[0]:.4f} k2e={me[1]:.4f} k3e={me[2]:.4f} cxe={me[3]:.4f} cye={me[4]:.4f}\")\n    history.append({'epoch': epoch, 'train_loss': t_loss/nb, 'val_loss': v_loss})\n\n    if v_loss < best_val_loss:\n        best_val_loss = v_loss\n        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n                    'val_loss': best_val_loss}, CKPT_DIR / 'best_model.pth')\n        print(f\"  >> Saved best (val_loss={best_val_loss:.4f})\")\n\nprint(f\"\\nDone. Best val_loss: {best_val_loss:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([h['epoch'] for h in history], [h['train_loss'] for h in history], label='Train')\n",
    "ax.plot([h['epoch'] for h in history], [h['val_loss'] for h in history], label='Val')\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.legend(); ax.set_title('Training Curves')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 — Local Scoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def edge_similarity(img1, img2, scales=(1.0, 0.5, 0.25)):\n",
    "    g1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) if len(img1.shape)==3 else img1\n",
    "    g2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) if len(img2.shape)==3 else img2\n",
    "    f1s = []\n",
    "    for s in scales:\n",
    "        a = cv2.resize(g1, (int(g1.shape[1]*s), int(g1.shape[0]*s))) if s != 1.0 else g1\n",
    "        b = cv2.resize(g2, (int(g2.shape[1]*s), int(g2.shape[0]*s))) if s != 1.0 else g2\n",
    "        e1 = cv2.Canny(a, int(max(0,0.67*np.median(a))), int(min(255,1.33*np.median(a))))\n",
    "        e2 = cv2.Canny(b, int(max(0,0.67*np.median(b))), int(min(255,1.33*np.median(b))))\n",
    "        k = np.ones((3,3), np.uint8)\n",
    "        e1d, e2d = cv2.dilate(e1,k,iterations=1), cv2.dilate(e2,k,iterations=1)\n",
    "        if e1.sum()==0 and e2.sum()==0: f1s.append(1.0); continue\n",
    "        if e1.sum()==0 or e2.sum()==0: f1s.append(0.0); continue\n",
    "        p = (e1 & e2d).sum()/max(e1.sum(),1)\n",
    "        r = (e2 & e1d).sum()/max(e2.sum(),1)\n",
    "        f1s.append(2*p*r/(p+r) if p+r>0 else 0.0)\n",
    "    return np.mean(f1s)\n",
    "\n",
    "def line_straightness(img, ref=None):\n",
    "    g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape)==3 else img\n",
    "    lines = cv2.HoughLinesP(cv2.Canny(g,50,150,apertureSize=3), 1, np.pi/180, 50, minLineLength=30, maxLineGap=10)\n",
    "    if lines is None: return 0.5\n",
    "    angles = np.array([np.arctan2(l[0][3]-l[0][1], l[0][2]-l[0][0])*180/np.pi for l in lines])\n",
    "    if ref is not None:\n",
    "        gr = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY) if len(ref.shape)==3 else ref\n",
    "        lr = cv2.HoughLinesP(cv2.Canny(gr,50,150,apertureSize=3), 1, np.pi/180, 50, minLineLength=30, maxLineGap=10)\n",
    "        if lr is None: return 0.5\n",
    "        ar = np.array([np.arctan2(l[0][3]-l[0][1], l[0][2]-l[0][0])*180/np.pi for l in lr])\n",
    "        bins = np.linspace(-90,90,37)\n",
    "        h1,_ = np.histogram(angles,bins=bins,density=True)\n",
    "        h2,_ = np.histogram(ar,bins=bins,density=True)\n",
    "        h1, h2 = h1/(h1.sum()+1e-10), h2/(h2.sum()+1e-10)\n",
    "        return float(np.sum(np.sqrt(h1*h2)))\n",
    "    return float(np.mean(np.minimum(np.abs(angles), np.abs(np.abs(angles)-90))<5))\n",
    "\n",
    "def gradient_orientation_sim(img1, img2, n_bins=36):\n",
    "    g1 = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY).astype(np.float32) if len(img1.shape)==3 else img1.astype(np.float32)\n",
    "    g2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY).astype(np.float32) if len(img2.shape)==3 else img2.astype(np.float32)\n",
    "    gx1, gy1 = cv2.Sobel(g1,cv2.CV_32F,1,0,ksize=3), cv2.Sobel(g1,cv2.CV_32F,0,1,ksize=3)\n",
    "    gx2, gy2 = cv2.Sobel(g2,cv2.CV_32F,1,0,ksize=3), cv2.Sobel(g2,cv2.CV_32F,0,1,ksize=3)\n",
    "    bins = np.linspace(-np.pi, np.pi, n_bins+1)\n",
    "    h1,_ = np.histogram(np.arctan2(gy1,gx1), bins=bins, weights=np.sqrt(gx1**2+gy1**2))\n",
    "    h2,_ = np.histogram(np.arctan2(gy2,gx2), bins=bins, weights=np.sqrt(gx2**2+gy2**2))\n",
    "    h1, h2 = h1/(h1.sum()+1e-10), h2/(h2.sum()+1e-10)\n",
    "    return float(np.sum(np.sqrt(h1*h2)))\n",
    "\n",
    "def pixel_accuracy(img1, img2):\n",
    "    if img1.shape != img2.shape: img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))\n",
    "    return 1.0 - np.mean(np.abs(img1.astype(np.float32) - img2.astype(np.float32)))/255.0\n",
    "\n",
    "def compute_score(corrected, gt):\n",
    "    if corrected.shape != gt.shape: corrected = cv2.resize(corrected, (gt.shape[1], gt.shape[0]))\n",
    "    es = edge_similarity(corrected, gt)\n",
    "    ls = line_straightness(corrected, gt)\n",
    "    go = gradient_orientation_sim(corrected, gt)\n",
    "    ss = ssim(corrected, gt, channel_axis=2, data_range=255) if len(corrected.shape)==3 else ssim(corrected, gt, data_range=255)\n",
    "    pa = pixel_accuracy(corrected, gt)\n",
    "    overall = 0.40*es + 0.22*ls + 0.18*go + 0.15*ss + 0.05*pa\n",
    "    return overall, dict(edge=es, line=ls, grad=go, ssim=ss, pixel=pa, overall=overall)\n",
    "\n",
    "print(\"Metrics defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9 — Validate on Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "ckpt = torch.load(CKPT_DIR / 'best_model.pth', map_location=DEVICE, weights_only=False)\nmodel.load_state_dict(ckpt['model_state_dict'])\nmodel.eval()\nprint(f\"Loaded best model from epoch {ckpt['epoch']}\")\n\nN_VIS = min(8, len(val_ds_noaug))\nval_vis_loader = DataLoader(val_ds_noaug, batch_size=1, shuffle=False)\n\nscores = []\nfig, axes = plt.subplots(N_VIS, 3, figsize=(15, 4*N_VIS))\nif N_VIS == 1: axes = [axes]\n\nfor i, batch in enumerate(val_vis_loader):\n    if i >= N_VIS: break\n    image_id = batch['image_id'][0]\n    with torch.no_grad():\n        pp = model(batch['image'].to(DEVICE)).cpu().numpy()[0]\n    orig_path, gen_path = PAIR_LOOKUP[image_id]\n    dist_img = cv2.imread(str(orig_path))\n    corr_img = cv2.imread(str(gen_path))\n    pred_corr = undistort_image(dist_img, *pp)\n    overall, m = compute_score(pred_corr, corr_img)\n    scores.append(overall)\n    axes[i][0].imshow(cv2.cvtColor(dist_img, cv2.COLOR_BGR2RGB))\n    axes[i][0].set_title('Distorted'); axes[i][0].axis('off')\n    axes[i][1].imshow(cv2.cvtColor(pred_corr, cv2.COLOR_BGR2RGB))\n    axes[i][1].set_title(f'Ours (score={overall:.3f})'); axes[i][1].axis('off')\n    axes[i][2].imshow(cv2.cvtColor(corr_img, cv2.COLOR_BGR2RGB))\n    axes[i][2].set_title('Ground truth'); axes[i][2].axis('off')\n\nplt.tight_layout(); plt.show()\nprint(f\"\\nAvg local score: {np.mean(scores):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10 — Predict Test + Test-Time Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "OUTPUT_DIR = WORK_DIR / 'output'\nOUTPUT_DIR.mkdir(exist_ok=True)\n\nckpt = torch.load(CKPT_DIR / 'best_model.pth', map_location=DEVICE, weights_only=False)\nmodel.load_state_dict(ckpt['model_state_dict'])\nmodel.eval()\n\ntest_ds = TestDataset(TEST_FILES, image_size=384)\ntest_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\nprint(f\"Test images: {len(test_ds)}\")\n\n# Stage 1: CNN predictions\npredictions = {}\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"CNN prediction\"):\n        preds = model(batch['image'].to(DEVICE))\n        for i, img_id in enumerate(batch['image_id']):\n            predictions[img_id] = {\n                'params': preds[i].cpu().numpy(),\n                'image_path': batch['image_path'][i],\n                'orig_h': batch['orig_h'][i].item(),\n                'orig_w': batch['orig_w'][i].item()}\n\n# Stage 2: Test-Time Optimization\nTTO_STEPS = 50\n\ndef tto_loss(undistorted):\n    img = undistorted.squeeze(0)\n    gray = (0.299*img[0]+0.587*img[1]+0.114*img[2]).unsqueeze(0).unsqueeze(0)\n    sx = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]],dtype=torch.float32,device=img.device).view(1,1,3,3)\n    sy = torch.tensor([[-1,-2,-1],[0,0,0],[1,2,1]],dtype=torch.float32,device=img.device).view(1,1,3,3)\n    gx, gy = F.conv2d(gray,sx,padding=1), F.conv2d(gray,sy,padding=1)\n    edge_loss = -torch.sqrt(gx**2+gy**2+1e-6).mean()\n    C,H,W = img.shape\n    bs = max(2, min(H,W)//20)\n    borders = torch.cat([img[:,:bs,:].reshape(-1), img[:,-bs:,:].reshape(-1),\n                         img[:,:,:bs].reshape(-1), img[:,:,-bs:].reshape(-1)])\n    return 0.5*edge_loss + 0.5*(1.0-borders.abs().clamp(0,1)).mean()\n\ndef diff_undistort(image, params):\n    B,C,H,W = image.shape\n    k1,k2,k3,cx,cy = params[0,0],params[0,1],params[0,2],params[0,3],params[0,4]\n    gy,gx = torch.meshgrid(torch.linspace(-1,1,H,device=image.device),\n                           torch.linspace(-1,1,W,device=image.device),indexing='ij')\n    dx, dy = gx-(cx*2-1), gy-(cy*2-1)\n    r2 = dx**2+dy**2\n    rad = 1+k1*r2+k2*r2**2+k3*r2**3\n    grid = torch.stack([dx*rad+(cx*2-1), dy*rad+(cy*2-1)], dim=-1).unsqueeze(0)\n    return F.grid_sample(image, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n\nprint(f\"\\nRunning TTO ({TTO_STEPS} steps/image)...\")\nfor img_id in tqdm(predictions, desc=\"TTO\"):\n    pred = predictions[img_id]\n    img = cv2.cvtColor(cv2.imread(pred['image_path']), cv2.COLOR_BGR2RGB)\n    img_t = torch.from_numpy(cv2.resize(img,(256,256))).float().permute(2,0,1).unsqueeze(0).to(DEVICE)/255.0\n    p = torch.tensor(pred['params'],dtype=torch.float32,device=DEVICE).unsqueeze(0).clone().detach().requires_grad_(True)\n    init_p = torch.tensor(pred['params'],dtype=torch.float32,device=DEVICE)\n    opt = torch.optim.Adam([p], lr=0.001)\n    best_loss, best_p = float('inf'), pred['params'].copy()\n    for _ in range(TTO_STEPS):\n        opt.zero_grad()\n        with torch.no_grad(): p.data[:,:3].clamp_(-1,1); p.data[:,3:].clamp_(0.1,0.9)\n        loss = tto_loss(diff_undistort(img_t, p)) + 0.1*F.mse_loss(p.squeeze(), init_p)\n        loss.backward(); opt.step()\n        if loss.item() < best_loss:\n            best_loss = loss.item()\n            best_p = p.detach().squeeze().cpu().numpy().copy()\n    pred['params_tto'] = best_p\n\nprint(\"TTO complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 — Apply Corrections & Create Submission ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "for img_id, pred in tqdm(predictions.items(), desc=\"Saving corrected\"):\n    img = cv2.imread(pred['image_path'])\n    if img is None: continue\n    corrected = undistort_image(img, *pred.get('params_tto', pred['params']))\n    cv2.imwrite(str(OUTPUT_DIR / f\"{img_id}.jpg\"), corrected, [cv2.IMWRITE_JPEG_QUALITY, 95])\n\nZIP_PATH = WORK_DIR / 'submission.zip'\nwith zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n    for f in sorted(OUTPUT_DIR.glob('*.jpg')): zf.write(f, f.name)\n\nn_out = len(list(OUTPUT_DIR.glob('*.jpg')))\nprint(f\"\\nSaved {n_out} corrected images\")\nprint(f\"ZIP: {ZIP_PATH} ({ZIP_PATH.stat().st_size/1024/1024:.1f} MB)\")\n\n# Auto-download on Colab\nif ENV == \"colab\":\n    from google.colab import files\n    files.download(str(ZIP_PATH))\n    print(\"Download started automatically.\")\nelif ENV == \"kaggle\":\n    print(\"Download submission.zip from the Output tab (right sidebar).\")\nelse:\n    print(f\"submission.zip is at: {ZIP_PATH}\")\n\nprint(f\"\\nNext steps:\")\nprint(f\"  1. Upload submission.zip to https://bounty.autohdr.com\")\nprint(f\"  2. Download the scoring CSV\")\nprint(f\"  3. Submit CSV to Kaggle\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12 — (Optional) Visualize Test Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_files = sorted(OUTPUT_DIR.glob('*.jpg'))[:6]\n",
    "fig, axes = plt.subplots(len(test_files), 2, figsize=(12, 4*len(test_files)))\n",
    "if len(test_files) == 1: axes = [axes]\n",
    "for i, cf in enumerate(test_files):\n",
    "    img_id = cf.stem\n",
    "    orig = cv2.cvtColor(cv2.imread(predictions[img_id]['image_path']), cv2.COLOR_BGR2RGB)\n",
    "    corr = cv2.cvtColor(cv2.imread(str(cf)), cv2.COLOR_BGR2RGB)\n",
    "    axes[i][0].imshow(orig); axes[i][0].set_title(f'Distorted: {img_id}'); axes[i][0].axis('off')\n",
    "    axes[i][1].imshow(corr); axes[i][1].set_title('Corrected'); axes[i][1].axis('off')\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}