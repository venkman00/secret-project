{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Lens Correction\n",
    "\n",
    "**Approach:** Parametric distortion prediction (k1, k2, k3, cx, cy) using EfficientNet-B3 + differentiable undistortion + test-time optimization.\n",
    "\n",
    "**Setup:** Add the competition data via **Add Data → Competition → automatic-lens-correction**. Enable **GPU** in accelerator settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1 — Install Dependencies & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q timm kornia pytorch-msssim albumentations\n",
    "\n",
    "import os, glob, json, csv, zipfile\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from scipy.optimize import minimize\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm.auto import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Explore data directory\n",
    "INPUT_DIR = Path('/kaggle/input/automatic-lens-correction')\n",
    "WORK_DIR = Path('/kaggle/working')\n",
    "\n",
    "print(f\"\\nData directory contents:\")\n",
    "for p in sorted(INPUT_DIR.rglob('*')):\n",
    "    if p.is_dir():\n",
    "        n_files = len(list(p.iterdir()))\n",
    "        print(f\"  [DIR]  {p.relative_to(INPUT_DIR)}/ ({n_files} items)\")\n",
    "\n",
    "# Count images\n",
    "img_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "all_images = [p for p in INPUT_DIR.rglob('*') if p.suffix.lower() in img_exts]\n",
    "print(f\"\\nTotal images found: {len(all_images)}\")\n",
    "\n",
    "# Show a few sample filenames\n",
    "print(\"\\nSample filenames:\")\n",
    "for img in all_images[:5]:\n",
    "    print(f\"  {img.relative_to(INPUT_DIR)}\")\n",
    "\n",
    "# Check resolution of first image\n",
    "if all_images:\n",
    "    sample = cv2.imread(str(all_images[0]))\n",
    "    if sample is not None:\n",
    "        print(f\"\\nSample resolution: {sample.shape[1]}x{sample.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 — Auto-Detect Directory Structure\n",
    "\n",
    "Run this cell, then **check the output** and adjust `DIST_DIR`, `CORR_DIR`, `TEST_DIR` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_dirs(data_dir):\n",
    "    \"\"\"Auto-detect distorted, corrected, and test directories.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    patterns = [\n",
    "        ('train/distorted', 'train/corrected'),\n",
    "        ('train/input', 'train/target'),\n",
    "        ('train_distorted', 'train_corrected'),\n",
    "        ('distorted', 'corrected'),\n",
    "        ('input', 'target'),\n",
    "        ('train_input', 'train_target'),\n",
    "        ('train/input', 'train/ground_truth'),\n",
    "    ]\n",
    "    \n",
    "    dist_dir = corr_dir = test_dir = None\n",
    "    \n",
    "    for dd, cd in patterns:\n",
    "        d = data_dir / dd\n",
    "        c = data_dir / cd\n",
    "        if d.exists() and c.exists():\n",
    "            dist_dir, corr_dir = d, c\n",
    "            break\n",
    "    \n",
    "    # Find test dir\n",
    "    test_patterns = ['test', 'test_images', 'test_input', 'test/input', 'test/distorted']\n",
    "    for tp in test_patterns:\n",
    "        t = data_dir / tp\n",
    "        if t.exists():\n",
    "            test_dir = t\n",
    "            break\n",
    "    \n",
    "    # Fallback: auto-detect from subdirectories\n",
    "    if dist_dir is None:\n",
    "        subdirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])\n",
    "        for i, d1 in enumerate(subdirs):\n",
    "            for d2 in subdirs[i+1:]:\n",
    "                files1 = {f.stem for f in d1.glob('*') if f.suffix.lower() in img_exts}\n",
    "                files2 = {f.stem for f in d2.glob('*') if f.suffix.lower() in img_exts}\n",
    "                if len(files1 & files2) > 10:\n",
    "                    dist_dir, corr_dir = d1, d2\n",
    "                    break\n",
    "            if dist_dir:\n",
    "                break\n",
    "    \n",
    "    if test_dir is None:\n",
    "        # Any remaining directory that doesn't overlap with train\n",
    "        subdirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])\n",
    "        for d in subdirs:\n",
    "            if d != dist_dir and d != corr_dir and d.parent != dist_dir and d.parent != corr_dir:\n",
    "                imgs = [f for f in d.rglob('*') if f.suffix.lower() in img_exts]\n",
    "                if imgs:\n",
    "                    test_dir = d\n",
    "                    break\n",
    "    \n",
    "    return dist_dir, corr_dir, test_dir\n",
    "\n",
    "DIST_DIR, CORR_DIR, TEST_DIR = find_dirs(INPUT_DIR)\n",
    "\n",
    "print(f\"Distorted dir: {DIST_DIR}\")\n",
    "print(f\"Corrected dir: {CORR_DIR}\")\n",
    "print(f\"Test dir:      {TEST_DIR}\")\n",
    "\n",
    "if DIST_DIR:\n",
    "    dist_imgs = [f for f in DIST_DIR.rglob('*') if f.suffix.lower() in img_exts]\n",
    "    print(f\"\\nDistorted images: {len(dist_imgs)}\")\n",
    "if CORR_DIR:\n",
    "    corr_imgs = [f for f in CORR_DIR.rglob('*') if f.suffix.lower() in img_exts]\n",
    "    print(f\"Corrected images: {len(corr_imgs)}\")\n",
    "if TEST_DIR:\n",
    "    test_imgs = [f for f in TEST_DIR.rglob('*') if f.suffix.lower() in img_exts]\n",
    "    print(f\"Test images:      {len(test_imgs)}\")\n",
    "\n",
    "# =====================================================\n",
    "# >>> MANUAL OVERRIDE: Uncomment and edit if auto-detect fails <<<\n",
    "# DIST_DIR = INPUT_DIR / 'train' / 'distorted'\n",
    "# CORR_DIR = INPUT_DIR / 'train' / 'corrected'\n",
    "# TEST_DIR = INPUT_DIR / 'test'\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — Visualize Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_pairs(dist_dir, corr_dir, n=4):\n",
    "    dist_files = sorted([f for f in Path(dist_dir).iterdir() if f.suffix.lower() in img_exts])[:n]\n",
    "    fig, axes = plt.subplots(n, 2, figsize=(12, 4*n))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for i, df in enumerate(dist_files):\n",
    "        for ext in img_exts:\n",
    "            cf = Path(corr_dir) / (df.stem + ext)\n",
    "            if cf.exists():\n",
    "                break\n",
    "        d_img = cv2.cvtColor(cv2.imread(str(df)), cv2.COLOR_BGR2RGB)\n",
    "        c_img = cv2.cvtColor(cv2.imread(str(cf)), cv2.COLOR_BGR2RGB)\n",
    "        axes[i][0].imshow(d_img)\n",
    "        axes[i][0].set_title(f'Distorted: {df.name}')\n",
    "        axes[i][0].axis('off')\n",
    "        axes[i][1].imshow(c_img)\n",
    "        axes[i][1].set_title(f'Corrected: {cf.name}')\n",
    "        axes[i][1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if DIST_DIR and CORR_DIR:\n",
    "    show_pairs(DIST_DIR, CORR_DIR, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — Extract Distortion Parameters from Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PARAMS_CSV = WORK_DIR / 'params.csv'\n",
    "\n",
    "def undistort_image(img, k1, k2, k3, cx, cy):\n",
    "    h, w = img.shape[:2]\n",
    "    fx = fy = max(h, w)\n",
    "    camera_matrix = np.array([[fx, 0, cx * w], [0, fy, cy * h], [0, 0, 1]], dtype=np.float64)\n",
    "    dist_coeffs = np.array([k1, k2, 0, 0, k3], dtype=np.float64)\n",
    "    new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(camera_matrix, dist_coeffs, (w, h), alpha=0)\n",
    "    undistorted = cv2.undistort(img, camera_matrix, dist_coeffs, None, new_camera_matrix)\n",
    "    x, y, rw, rh = roi\n",
    "    if rw > 0 and rh > 0:\n",
    "        undistorted = undistorted[y:y+rh, x:x+rw]\n",
    "        undistorted = cv2.resize(undistorted, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    return undistorted\n",
    "\n",
    "def objective(params, distorted, corrected):\n",
    "    k1, k2, k3, cx, cy = params\n",
    "    try:\n",
    "        undist = undistort_image(distorted, k1, k2, k3, cx, cy)\n",
    "        return np.mean((undist.astype(np.float32) - corrected.astype(np.float32)) ** 2)\n",
    "    except Exception:\n",
    "        return 1e10\n",
    "\n",
    "def extract_single(args):\n",
    "    dist_path, corr_path, size = args\n",
    "    dist_img = cv2.imread(str(dist_path))\n",
    "    corr_img = cv2.imread(str(corr_path))\n",
    "    if dist_img is None or corr_img is None:\n",
    "        return None\n",
    "    dist_small = cv2.resize(dist_img, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    corr_small = cv2.resize(corr_img, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    x0 = np.array([0.0, 0.0, 0.0, 0.5, 0.5])\n",
    "    bounds = [(-1.0, 1.0), (-1.0, 1.0), (-1.0, 1.0), (0.3, 0.7), (0.3, 0.7)]\n",
    "    result = minimize(objective, x0, args=(dist_small, corr_small),\n",
    "                      method='L-BFGS-B', bounds=bounds, options={'maxiter': 200, 'ftol': 1e-8})\n",
    "    return result.x\n",
    "\n",
    "# Match training pairs\n",
    "dist_files = sorted([f for f in DIST_DIR.iterdir() if f.suffix.lower() in img_exts])\n",
    "pairs = []\n",
    "for df in dist_files:\n",
    "    for ext in img_exts:\n",
    "        cf = CORR_DIR / (df.stem + ext)\n",
    "        if cf.exists():\n",
    "            pairs.append((df, cf))\n",
    "            break\n",
    "\n",
    "print(f\"Matched {len(pairs)} training pairs\")\n",
    "\n",
    "# Extract params in parallel\n",
    "OPT_SIZE = 256\n",
    "N_WORKERS = 4\n",
    "\n",
    "results = []\n",
    "work_items = [(d, c, OPT_SIZE) for d, c in pairs]\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=N_WORKERS) as executor:\n",
    "    futures = {executor.submit(extract_single, w): w[0].stem for w in work_items}\n",
    "    for f in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting params\"):\n",
    "        image_id = futures[f]\n",
    "        try:\n",
    "            params = f.result()\n",
    "            if params is not None:\n",
    "                results.append((image_id, *params))\n",
    "        except Exception as e:\n",
    "            print(f\"Error {image_id}: {e}\")\n",
    "\n",
    "# Save CSV\n",
    "with open(PARAMS_CSV, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['image_id', 'k1', 'k2', 'k3', 'cx', 'cy'])\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"\\nSaved {len(results)} parameter sets to {PARAMS_CSV}\")\n",
    "\n",
    "# Quick validation\n",
    "print(\"\\nValidation (PSNR on full-res):\")\n",
    "param_dict = {r[0]: r[1:] for r in results}\n",
    "psnrs = []\n",
    "for dp, cp in pairs[:5]:\n",
    "    if dp.stem not in param_dict:\n",
    "        continue\n",
    "    k1, k2, k3, cx, cy = param_dict[dp.stem]\n",
    "    d = cv2.imread(str(dp))\n",
    "    c = cv2.imread(str(cp))\n",
    "    u = undistort_image(d, k1, k2, k3, cx, cy)\n",
    "    mse = np.mean((u.astype(np.float32) - c.astype(np.float32)) ** 2)\n",
    "    psnr = 10 * np.log10(255**2 / max(mse, 1e-10))\n",
    "    psnrs.append(psnr)\n",
    "    print(f\"  {dp.stem}: PSNR = {psnr:.2f} dB\")\n",
    "if psnrs:\n",
    "    print(f\"  Average: {np.mean(psnrs):.2f} dB (target: >30 dB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — Dataset & Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DistortionDataset(Dataset):\n",
    "    def __init__(self, image_dir, params_csv, image_size=224, augment=True, corrected_dir=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.corrected_dir = Path(corrected_dir) if corrected_dir else None\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "\n",
    "        self.samples = []\n",
    "        with open(params_csv, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                params = np.array([float(row['k1']), float(row['k2']), float(row['k3']),\n",
    "                                   float(row['cx']), float(row['cy'])], dtype=np.float32)\n",
    "                self.samples.append((row['image_id'], params))\n",
    "\n",
    "        self._image_paths = {}\n",
    "        for f in self.image_dir.iterdir():\n",
    "            if f.suffix.lower() in img_exts:\n",
    "                self._image_paths[f.stem] = f\n",
    "        self.samples = [(i, p) for i, p in self.samples if i in self._image_paths]\n",
    "        self._build_transforms()\n",
    "\n",
    "    def _build_transforms(self):\n",
    "        sz = self.image_size\n",
    "        if self.augment:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(sz, sz), A.ColorJitter(0.2, 0.2, 0.1, 0.05, p=0.5),\n",
    "                A.GaussNoise(std_range=(0.01, 0.03), p=0.3),\n",
    "                A.ImageCompression(quality_range=(70, 95), p=0.3),\n",
    "                A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()])\n",
    "        else:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(sz, sz),\n",
    "                A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()])\n",
    "        self.target_transform = A.Compose([\n",
    "            A.Resize(sz, sz),\n",
    "            A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()])\n",
    "\n",
    "    def update_image_size(self, new_size):\n",
    "        self.image_size = new_size\n",
    "        self._build_transforms()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, params = self.samples[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(str(self._image_paths[image_id])), cv2.COLOR_BGR2RGB)\n",
    "        result = {'image': self.transform(image=img)['image'],\n",
    "                  'params': torch.from_numpy(params), 'image_id': image_id}\n",
    "        if self.corrected_dir is not None:\n",
    "            for ext in img_exts:\n",
    "                cp = self.corrected_dir / (image_id + ext)\n",
    "                if cp.exists():\n",
    "                    ci = cv2.cvtColor(cv2.imread(str(cp)), cv2.COLOR_BGR2RGB)\n",
    "                    result['corrected'] = self.target_transform(image=ci)['image']\n",
    "                    break\n",
    "        return result\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_size=384):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_files = sorted([f for f in self.image_dir.iterdir() if f.suffix.lower() in img_exts])\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]), ToTensorV2()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_files[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(str(p)), cv2.COLOR_BGR2RGB)\n",
    "        oh, ow = img.shape[:2]\n",
    "        return {'image': self.transform(image=img)['image'], 'image_id': p.stem,\n",
    "                'image_path': str(p), 'orig_h': oh, 'orig_w': ow}\n",
    "\n",
    "\n",
    "class DistortionNet(nn.Module):\n",
    "    def __init__(self, backbone='efficientnet_b3', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained, num_classes=0)\n",
    "        feat_dim = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2), nn.Linear(256, 5))\n",
    "        nn.init.zeros_(self.head[-1].weight)\n",
    "        nn.init.zeros_(self.head[-1].bias)\n",
    "        with torch.no_grad():\n",
    "            self.head[-1].bias[3] = 0.5\n",
    "            self.head[-1].bias[4] = 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        params = self.head(features)\n",
    "        k = torch.tanh(params[:, :3])\n",
    "        center = torch.sigmoid(params[:, 3:])\n",
    "        return torch.cat([k, center], dim=1)\n",
    "\n",
    "\n",
    "def differentiable_undistort(image, params, output_size=None):\n",
    "    B, C, H, W = image.shape\n",
    "    out_H, out_W = output_size if output_size else (H, W)\n",
    "    k1, k2, k3 = params[:, 0:1], params[:, 1:2], params[:, 2:3]\n",
    "    cx, cy = params[:, 3:4], params[:, 4:5]\n",
    "\n",
    "    gy, gx = torch.meshgrid(\n",
    "        torch.linspace(-1, 1, out_H, device=image.device),\n",
    "        torch.linspace(-1, 1, out_W, device=image.device), indexing='ij')\n",
    "    gx = gx.unsqueeze(0).expand(B, -1, -1)\n",
    "    gy = gy.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    cx_n = (cx * 2 - 1).unsqueeze(-1)\n",
    "    cy_n = (cy * 2 - 1).unsqueeze(-1)\n",
    "    dx = gx - cx_n\n",
    "    dy = gy - cy_n\n",
    "    r2 = dx**2 + dy**2\n",
    "\n",
    "    k1, k2, k3 = k1.unsqueeze(-1), k2.unsqueeze(-1), k3.unsqueeze(-1)\n",
    "    radial = 1 + k1 * r2 + k2 * r2**2 + k3 * r2**3\n",
    "    grid = torch.stack([dx * radial + cx_n, dy * radial + cy_n], dim=-1)\n",
    "    return F.grid_sample(image, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "\n",
    "class DistortionLoss(nn.Module):\n",
    "    def __init__(self, param_weight=1.0, pixel_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.pw, self.xw = param_weight, pixel_weight\n",
    "\n",
    "    def forward(self, pred, gt, dist_img=None, corr_img=None):\n",
    "        lp = F.mse_loss(pred, gt)\n",
    "        lx = torch.tensor(0.0, device=pred.device)\n",
    "        if dist_img is not None and corr_img is not None:\n",
    "            pred_corr = differentiable_undistort(dist_img, pred)\n",
    "            lx = F.l1_loss(pred_corr, corr_img)\n",
    "        return self.pw * lp + self.xw * lx, lp, lx\n",
    "\n",
    "\n",
    "print(f\"Dataset & model classes defined.\")\n",
    "model = DistortionNet(pretrained=True).to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ── Hyperparameters ──\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "LR = 3e-3\n",
    "WEIGHT_DECAY = 0.01\n",
    "VAL_SPLIT = 0.2\n",
    "NUM_WORKERS = 2\n",
    "CKPT_DIR = WORK_DIR / 'checkpoints'\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Progressive resizing schedule\n",
    "SIZE_SCHEDULE = {0: 224, EPOCHS // 3: 384, 2 * EPOCHS // 3: 512}\n",
    "\n",
    "# ── Dataset & Split ──\n",
    "full_ds = DistortionDataset(DIST_DIR, PARAMS_CSV, image_size=224, augment=True, corrected_dir=CORR_DIR)\n",
    "n_val = int(len(full_ds) * VAL_SPLIT)\n",
    "n_train = len(full_ds) - n_val\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "val_ds_noaug = DistortionDataset(DIST_DIR, PARAMS_CSV, image_size=224, augment=False, corrected_dir=CORR_DIR)\n",
    "val_ds_noaug.samples = [val_ds_noaug.samples[i] for i in val_ds.indices]\n",
    "\n",
    "print(f\"Train: {n_train}, Val: {n_val}\")\n",
    "\n",
    "# ── Model, Loss, Optimizer ──\n",
    "model = DistortionNet(pretrained=True).to(DEVICE)\n",
    "criterion = DistortionLoss(param_weight=1.0, pixel_weight=0.5)\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "steps_per_epoch = n_train // BATCH_SIZE + 1\n",
    "scheduler = OneCycleLR(optimizer, max_lr=LR, total_steps=steps_per_epoch * EPOCHS,\n",
    "                       pct_start=0.3, div_factor=25, final_div_factor=1000)\n",
    "\n",
    "# ── Training Loop ──\n",
    "best_val_loss = float('inf')\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Progressive resize\n",
    "    if epoch - 1 in SIZE_SCHEDULE:\n",
    "        sz = SIZE_SCHEDULE[epoch - 1]\n",
    "        print(f\"\\n>>> Resize to {sz}x{sz}\")\n",
    "        full_ds.update_image_size(sz)\n",
    "        val_ds_noaug.update_image_size(sz)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds_noaug, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    t_loss, t_pl, t_xl, nb = 0, 0, 0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "    for batch in pbar:\n",
    "        imgs = batch['image'].to(DEVICE)\n",
    "        params = batch['params'].to(DEVICE)\n",
    "        corr = batch.get('corrected')\n",
    "        corr = corr.to(DEVICE) if corr is not None else None\n",
    "\n",
    "        pred = model(imgs)\n",
    "        loss, lp, lx = criterion(pred, params, imgs, corr)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        t_loss += loss.item(); t_pl += lp.item(); t_xl += lx.item(); nb += 1\n",
    "        pbar.set_postfix(loss=f\"{t_loss/nb:.4f}\", param=f\"{t_pl/nb:.4f}\", pixel=f\"{t_xl/nb:.4f}\")\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    v_loss, v_pl, errs = 0, 0, []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            imgs = batch['image'].to(DEVICE)\n",
    "            params = batch['params'].to(DEVICE)\n",
    "            corr = batch.get('corrected')\n",
    "            corr = corr.to(DEVICE) if corr is not None else None\n",
    "            pred = model(imgs)\n",
    "            loss, lp, lx = criterion(pred, params, imgs, corr)\n",
    "            v_loss += loss.item(); v_pl += lp.item()\n",
    "            errs.append((pred - params).abs().cpu().numpy())\n",
    "\n",
    "    n_vb = max(len(val_loader), 1)\n",
    "    v_loss /= n_vb\n",
    "    me = np.concatenate(errs).mean(axis=0) if errs else np.zeros(5)\n",
    "    print(f\"Epoch {epoch}: train={t_loss/nb:.4f} val={v_loss:.4f} | \"\n",
    "          f\"k1e={me[0]:.4f} k2e={me[1]:.4f} k3e={me[2]:.4f} cxe={me[3]:.4f} cye={me[4]:.4f}\")\n",
    "\n",
    "    history.append({'epoch': epoch, 'train_loss': t_loss/nb, 'val_loss': v_loss})\n",
    "\n",
    "    if v_loss < best_val_loss:\n",
    "        best_val_loss = v_loss\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "                    'val_loss': best_val_loss}, CKPT_DIR / 'best_model.pth')\n",
    "        print(f\"  >> Saved best model (val_loss={best_val_loss:.4f})\")\n",
    "\n",
    "print(f\"\\nDone. Best val_loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([h['epoch'] for h in history], [h['train_loss'] for h in history], label='Train')\n",
    "ax.plot([h['epoch'] for h in history], [h['val_loss'] for h in history], label='Val')\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.legend(); ax.set_title('Training Curves')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 — Metrics (Local Scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def edge_similarity(img1, img2, scales=(1.0, 0.5, 0.25)):\n",
    "    g1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) if len(img1.shape) == 3 else img1\n",
    "    g2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) if len(img2.shape) == 3 else img2\n",
    "    f1s = []\n",
    "    for s in scales:\n",
    "        if s != 1.0:\n",
    "            h, w = int(g1.shape[0]*s), int(g1.shape[1]*s)\n",
    "            a, b = cv2.resize(g1, (w,h)), cv2.resize(g2, (w,h))\n",
    "        else:\n",
    "            a, b = g1, g2\n",
    "        m1, m2 = np.median(a), np.median(b)\n",
    "        e1 = cv2.Canny(a, int(max(0,0.67*m1)), int(min(255,1.33*m1)))\n",
    "        e2 = cv2.Canny(b, int(max(0,0.67*m2)), int(min(255,1.33*m2)))\n",
    "        k = np.ones((3,3), np.uint8)\n",
    "        e1d, e2d = cv2.dilate(e1, k, iterations=1), cv2.dilate(e2, k, iterations=1)\n",
    "        if e1.sum() == 0 and e2.sum() == 0: f1s.append(1.0); continue\n",
    "        if e1.sum() == 0 or e2.sum() == 0: f1s.append(0.0); continue\n",
    "        p = (e1 & e2d).sum() / max(e1.sum(), 1)\n",
    "        r = (e2 & e1d).sum() / max(e2.sum(), 1)\n",
    "        f1s.append(2*p*r/(p+r) if p+r > 0 else 0.0)\n",
    "    return np.mean(f1s)\n",
    "\n",
    "def line_straightness(img, ref=None):\n",
    "    g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\n",
    "    e = cv2.Canny(g, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLinesP(e, 1, np.pi/180, 50, minLineLength=30, maxLineGap=10)\n",
    "    if lines is None: return 0.5\n",
    "    angles = np.array([np.arctan2(l[0][3]-l[0][1], l[0][2]-l[0][0])*180/np.pi for l in lines])\n",
    "    if ref is not None:\n",
    "        gr = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY) if len(ref.shape) == 3 else ref\n",
    "        er = cv2.Canny(gr, 50, 150, apertureSize=3)\n",
    "        lr = cv2.HoughLinesP(er, 1, np.pi/180, 50, minLineLength=30, maxLineGap=10)\n",
    "        if lr is None: return 0.5\n",
    "        ar = np.array([np.arctan2(l[0][3]-l[0][1], l[0][2]-l[0][0])*180/np.pi for l in lr])\n",
    "        bins = np.linspace(-90, 90, 37)\n",
    "        h1, _ = np.histogram(angles, bins=bins, density=True)\n",
    "        h2, _ = np.histogram(ar, bins=bins, density=True)\n",
    "        h1, h2 = h1/(h1.sum()+1e-10), h2/(h2.sum()+1e-10)\n",
    "        return float(np.sum(np.sqrt(h1*h2)))\n",
    "    return float(np.mean(np.minimum(np.abs(angles), np.abs(np.abs(angles)-90)) < 5))\n",
    "\n",
    "def gradient_orientation_similarity(img1, img2, n_bins=36):\n",
    "    g1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY).astype(np.float32) if len(img1.shape)==3 else img1.astype(np.float32)\n",
    "    g2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY).astype(np.float32) if len(img2.shape)==3 else img2.astype(np.float32)\n",
    "    gx1, gy1 = cv2.Sobel(g1,cv2.CV_32F,1,0,ksize=3), cv2.Sobel(g1,cv2.CV_32F,0,1,ksize=3)\n",
    "    gx2, gy2 = cv2.Sobel(g2,cv2.CV_32F,1,0,ksize=3), cv2.Sobel(g2,cv2.CV_32F,0,1,ksize=3)\n",
    "    bins = np.linspace(-np.pi, np.pi, n_bins+1)\n",
    "    h1, _ = np.histogram(np.arctan2(gy1,gx1), bins=bins, weights=np.sqrt(gx1**2+gy1**2))\n",
    "    h2, _ = np.histogram(np.arctan2(gy2,gx2), bins=bins, weights=np.sqrt(gx2**2+gy2**2))\n",
    "    h1, h2 = h1/(h1.sum()+1e-10), h2/(h2.sum()+1e-10)\n",
    "    return float(np.sum(np.sqrt(h1*h2)))\n",
    "\n",
    "def pixel_accuracy(img1, img2):\n",
    "    if img1.shape != img2.shape: img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))\n",
    "    return 1.0 - np.mean(np.abs(img1.astype(np.float32) - img2.astype(np.float32))) / 255.0\n",
    "\n",
    "def compute_score(corrected, gt):\n",
    "    if corrected.shape != gt.shape: corrected = cv2.resize(corrected, (gt.shape[1], gt.shape[0]))\n",
    "    es = edge_similarity(corrected, gt)\n",
    "    ls = line_straightness(corrected, gt)\n",
    "    go = gradient_orientation_similarity(corrected, gt)\n",
    "    ss = ssim(corrected, gt, channel_axis=2, data_range=255) if len(corrected.shape)==3 else ssim(corrected, gt, data_range=255)\n",
    "    pa = pixel_accuracy(corrected, gt)\n",
    "    overall = 0.40*es + 0.22*ls + 0.18*go + 0.15*ss + 0.05*pa\n",
    "    return overall, dict(edge=es, line=ls, grad=go, ssim=ss, pixel=pa, overall=overall)\n",
    "\n",
    "print(\"Metrics defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9 — Validate on Training Pairs (Local Metric Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Score a few validation images using our model + local metrics\n",
    "ckpt = torch.load(CKPT_DIR / 'best_model.pth', map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "print(f\"Loaded best model from epoch {ckpt['epoch']}\")\n",
    "\n",
    "N_VIS = min(8, len(val_ds_noaug))\n",
    "val_loader_vis = DataLoader(val_ds_noaug, batch_size=1, shuffle=False)\n",
    "\n",
    "scores = []\n",
    "fig, axes = plt.subplots(N_VIS, 3, figsize=(15, 4 * N_VIS))\n",
    "if N_VIS == 1: axes = [axes]\n",
    "\n",
    "for i, batch in enumerate(val_loader_vis):\n",
    "    if i >= N_VIS:\n",
    "        break\n",
    "    image_id = batch['image_id'][0]\n",
    "\n",
    "    # Predict params\n",
    "    with torch.no_grad():\n",
    "        pred_params = model(batch['image'].to(DEVICE)).cpu().numpy()[0]\n",
    "    k1, k2, k3, cx, cy = pred_params\n",
    "\n",
    "    # Load full-res images\n",
    "    dist_path = next(DIST_DIR.glob(f\"{image_id}.*\"))\n",
    "    corr_path = next(CORR_DIR.glob(f\"{image_id}.*\"))\n",
    "    dist_img = cv2.imread(str(dist_path))\n",
    "    corr_img = cv2.imread(str(corr_path))\n",
    "\n",
    "    # Undistort\n",
    "    pred_corr = undistort_image(dist_img, k1, k2, k3, cx, cy)\n",
    "\n",
    "    # Score\n",
    "    overall, m = compute_score(pred_corr, corr_img)\n",
    "    scores.append(overall)\n",
    "\n",
    "    # Visualize\n",
    "    axes[i][0].imshow(cv2.cvtColor(dist_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[i][0].set_title(f'Distorted'); axes[i][0].axis('off')\n",
    "    axes[i][1].imshow(cv2.cvtColor(pred_corr, cv2.COLOR_BGR2RGB))\n",
    "    axes[i][1].set_title(f'Our correction (score={overall:.3f})'); axes[i][1].axis('off')\n",
    "    axes[i][2].imshow(cv2.cvtColor(corr_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[i][2].set_title(f'Ground truth'); axes[i][2].axis('off')\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "print(f\"\\nAvg local score on {len(scores)} val images: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10 — Predict Test Images + Test-Time Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = WORK_DIR / 'output'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load best model\n",
    "ckpt = torch.load(CKPT_DIR / 'best_model.pth', map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "test_ds = TestDataset(TEST_DIR, image_size=384)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
    "print(f\"Test images: {len(test_ds)}\")\n",
    "\n",
    "# ── Stage 1: CNN predictions ──\n",
    "predictions = {}\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"CNN prediction\"):\n",
    "        imgs = batch['image'].to(DEVICE)\n",
    "        params = model(imgs)\n",
    "        for i, img_id in enumerate(batch['image_id']):\n",
    "            predictions[img_id] = {\n",
    "                'params': params[i].cpu().numpy(),\n",
    "                'image_path': batch['image_path'][i],\n",
    "                'orig_h': batch['orig_h'][i].item(),\n",
    "                'orig_w': batch['orig_w'][i].item(),\n",
    "            }\n",
    "\n",
    "# ── Stage 2: Test-Time Optimization ──\n",
    "TTO_STEPS = 50\n",
    "TTO_LR = 0.001\n",
    "\n",
    "def tto_loss(undistorted):\n",
    "    \"\"\"Self-supervised loss: edge sharpness + border coverage.\"\"\"\n",
    "    img = undistorted.squeeze(0)\n",
    "    gray = (0.299*img[0] + 0.587*img[1] + 0.114*img[2]).unsqueeze(0).unsqueeze(0)\n",
    "    sx = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=torch.float32, device=img.device).view(1,1,3,3)\n",
    "    sy = torch.tensor([[-1,-2,-1],[0,0,0],[1,2,1]], dtype=torch.float32, device=img.device).view(1,1,3,3)\n",
    "    gx, gy = F.conv2d(gray, sx, padding=1), F.conv2d(gray, sy, padding=1)\n",
    "    edge_loss = -torch.sqrt(gx**2 + gy**2 + 1e-6).mean()\n",
    "    C, H, W = img.shape\n",
    "    bs = max(2, min(H,W)//20)\n",
    "    borders = torch.cat([img[:,:bs,:].reshape(-1), img[:,-bs:,:].reshape(-1),\n",
    "                         img[:,:,:bs].reshape(-1), img[:,:,-bs:].reshape(-1)])\n",
    "    border_loss = (1.0 - borders.abs().clamp(0,1)).mean()\n",
    "    return 0.5*edge_loss + 0.5*border_loss\n",
    "\n",
    "def diff_undistort_single(image, params):\n",
    "    B, C, H, W = image.shape\n",
    "    k1, k2, k3, cx, cy = params[0,0], params[0,1], params[0,2], params[0,3], params[0,4]\n",
    "    gy, gx = torch.meshgrid(torch.linspace(-1,1,H,device=image.device),\n",
    "                            torch.linspace(-1,1,W,device=image.device), indexing='ij')\n",
    "    dx, dy = gx - (cx*2-1), gy - (cy*2-1)\n",
    "    r2 = dx**2 + dy**2\n",
    "    radial = 1 + k1*r2 + k2*r2**2 + k3*r2**3\n",
    "    grid = torch.stack([dx*radial + (cx*2-1), dy*radial + (cy*2-1)], dim=-1).unsqueeze(0)\n",
    "    return F.grid_sample(image, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "print(f\"\\nRunning TTO ({TTO_STEPS} steps per image)...\")\n",
    "for img_id in tqdm(predictions, desc=\"TTO\"):\n",
    "    pred = predictions[img_id]\n",
    "    img = cv2.cvtColor(cv2.imread(pred['image_path']), cv2.COLOR_BGR2RGB)\n",
    "    img_small = cv2.resize(img, (256, 256))\n",
    "    img_t = torch.from_numpy(img_small).float().permute(2,0,1).unsqueeze(0).to(DEVICE) / 255.0\n",
    "\n",
    "    p = torch.tensor(pred['params'], dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    p = p.clone().detach().requires_grad_(True)\n",
    "    init_p = torch.tensor(pred['params'], dtype=torch.float32, device=DEVICE)\n",
    "    opt = torch.optim.Adam([p], lr=TTO_LR)\n",
    "\n",
    "    best_loss, best_p = float('inf'), pred['params'].copy()\n",
    "    for _ in range(TTO_STEPS):\n",
    "        opt.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            p.data[:,:3].clamp_(-1,1); p.data[:,3:].clamp_(0.1,0.9)\n",
    "        undist = diff_undistort_single(img_t, p)\n",
    "        loss = tto_loss(undist) + 0.1 * F.mse_loss(p.squeeze(), init_p)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_p = p.detach().squeeze().cpu().numpy().copy()\n",
    "\n",
    "    pred['params_tto'] = best_p\n",
    "\n",
    "print(\"TTO complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 — Apply Corrections & Create Submission ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for img_id, pred in tqdm(predictions.items(), desc=\"Saving corrected images\"):\n",
    "    img = cv2.imread(pred['image_path'])\n",
    "    if img is None:\n",
    "        continue\n",
    "    params = pred.get('params_tto', pred['params'])\n",
    "    k1, k2, k3, cx, cy = params\n",
    "    corrected = undistort_image(img, k1, k2, k3, cx, cy)\n",
    "    cv2.imwrite(str(OUTPUT_DIR / f\"{img_id}.jpg\"), corrected, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
    "\n",
    "# Create ZIP\n",
    "ZIP_PATH = WORK_DIR / 'submission.zip'\n",
    "with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for img_file in sorted(OUTPUT_DIR.glob('*.jpg')):\n",
    "        zf.write(img_file, img_file.name)\n",
    "\n",
    "n_output = len(list(OUTPUT_DIR.glob('*.jpg')))\n",
    "zip_mb = ZIP_PATH.stat().st_size / 1024 / 1024\n",
    "print(f\"\\nSaved {n_output} corrected images\")\n",
    "print(f\"Submission ZIP: {ZIP_PATH} ({zip_mb:.1f} MB)\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Download submission.zip from the Output tab (right sidebar)\")\n",
    "print(f\"  2. Upload to https://bounty.autohdr.com\")\n",
    "print(f\"  3. Download the scoring CSV\")\n",
    "print(f\"  4. Submit CSV to Kaggle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12 — (Optional) Visualize Test Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_files = sorted(OUTPUT_DIR.glob('*.jpg'))[:6]\n",
    "fig, axes = plt.subplots(len(test_files), 2, figsize=(12, 4 * len(test_files)))\n",
    "if len(test_files) == 1: axes = [axes]\n",
    "\n",
    "for i, cf in enumerate(test_files):\n",
    "    img_id = cf.stem\n",
    "    orig = cv2.cvtColor(cv2.imread(predictions[img_id]['image_path']), cv2.COLOR_BGR2RGB)\n",
    "    corr = cv2.cvtColor(cv2.imread(str(cf)), cv2.COLOR_BGR2RGB)\n",
    "    axes[i][0].imshow(orig); axes[i][0].set_title(f'Distorted: {img_id}'); axes[i][0].axis('off')\n",
    "    axes[i][1].imshow(corr); axes[i][1].set_title(f'Corrected'); axes[i][1].axis('off')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
